{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "086f974e",
   "metadata": {},
   "source": [
    "# NO_ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c91ae545",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test_id:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stop at epoch 106 (sim 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test_id: 100%|██████████| 1/1 [00:12<00:00, 12.60s/it]\n"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyreadr\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# =========================\n",
    "# Project root \n",
    "# =========================\n",
    "def detect_project_root():\n",
    "    env = os.environ.get(\"CF_CONT_ROOT\")\n",
    "    if env:\n",
    "        return Path(env).expanduser().resolve()\n",
    "    try:\n",
    "        here = Path(__file__).resolve()\n",
    "    except NameError:\n",
    "        here = Path.cwd().resolve()\n",
    "    for p in [here] + list(here.parents):\n",
    "        if p.name == \"CF_Continuous\":\n",
    "            return p\n",
    "        if (p / \"data\" / \"data_20230504\").exists() and (p / \"codes\").exists():\n",
    "            return p\n",
    "    return here\n",
    "\n",
    "PROJ_ROOT = detect_project_root()\n",
    "\n",
    "# =========================\n",
    "# Paths \n",
    "# =========================\n",
    "SPLIT_DIR = PROJ_ROOT / \"data\" / \"train_test_split\"\n",
    "DATA_2ND_STAGE_RDS = PROJ_ROOT / \"data\" / \"data_20230504\" / \"data_2nd_stage.rds\"\n",
    "EVALL_N_SEQ_RDS    = PROJ_ROOT / \"data\" / \"data_20230504\" / \"evall_N_seq.rds\"\n",
    "\n",
    "# =========================\n",
    "# Output \n",
    "# =========================\n",
    "model_tag   = \"NO_ANN\"\n",
    "n_fields    = 20               # choose 1, 5, or 10\n",
    "run_test_id = 1                # run only this test_id\n",
    "fields_label_map = {1: \"one_field\", 5: \"five_fields\", 10: \"ten_fields\"}\n",
    "fields_label = fields_label_map.get(n_fields, f\"{n_fields}_fields\")\n",
    "\n",
    "RESULTS_BASE = PROJ_ROOT / \"results\" / \"yield_response_function_for_one_iteration\"\n",
    "RESULTS_DIR  = RESULTS_BASE / f\"YRF_{model_tag}_{fields_label}\"\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === Model: Simple ANN (NO ANN) ===\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(4, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# --- Load data ---\n",
    "data_2nd_stage = next(iter(pyreadr.read_r(str(DATA_2ND_STAGE_RDS)).values()))\n",
    "evall_N_seq    = next(iter(pyreadr.read_r(str(EVALL_N_SEQ_RDS)).values()))\n",
    "\n",
    "# Coerce numerics (include b1, b2 so we can compute true_yield via QP)\n",
    "for c in ['yield','Nk','plateau','b0','b1','b2','N']:\n",
    "    if c in data_2nd_stage.columns:\n",
    "        data_2nd_stage[c] = pd.to_numeric(data_2nd_stage[c], errors='coerce')\n",
    "for c in ['sim','N','N_tilde']:\n",
    "    if c in evall_N_seq.columns:\n",
    "        evall_N_seq[c] = pd.to_numeric(evall_N_seq[c], errors='coerce')\n",
    "\n",
    "# Clean\n",
    "data_2nd_stage = data_2nd_stage.dropna(subset=['yield','Nk','plateau','b0','N']).reset_index(drop=True)\n",
    "evall_N_seq    = evall_N_seq.dropna(subset=['N']).reset_index(drop=True)\n",
    "\n",
    "# --- Load split CSV and restrict to one test_id ---\n",
    "split_csv_path = SPLIT_DIR / f\"train_test_splits_{n_fields}fields.csv\"\n",
    "splits_df = pd.read_csv(split_csv_path)\n",
    "splits_df = splits_df[splits_df['test_id'] == run_test_id].iloc[:1].copy()\n",
    "\n",
    "# Features used for the ANN\n",
    "feature_cols = ['Nk', 'plateau', 'b0', 'N']\n",
    "\n",
    "# === Loop (only one iteration) ===\n",
    "for _, row in tqdm(splits_df.iterrows(), total=len(splits_df), desc=\"Processing test_id\"):\n",
    "    test_id = int(row['test_id'])\n",
    "    train_ids = row[[c for c in row.index if c.startswith('train_')]].values\n",
    "\n",
    "    # ------- Train/val -------\n",
    "    dataset = data_2nd_stage[data_2nd_stage['sim'].isin(train_ids)].reset_index(drop=True)\n",
    "    dataset = dataset[['yield'] + feature_cols].copy()\n",
    "\n",
    "    train_df = dataset.sample(frac=0.8, random_state=0)\n",
    "    val_df   = dataset.drop(train_df.index)\n",
    "\n",
    "    X_train = train_df.drop('yield', axis=1)\n",
    "    y_train = train_df['yield'].to_numpy().reshape(-1, 1)\n",
    "    X_val   = val_df.drop('yield', axis=1)\n",
    "    y_val   = val_df['yield'].to_numpy().reshape(-1, 1)\n",
    "\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_val_scaled   = scaler.transform(X_val)\n",
    "\n",
    "    X_train_t = torch.tensor(X_train_scaled, dtype=torch.float32, device=device)\n",
    "    y_train_t = torch.tensor(y_train,       dtype=torch.float32, device=device)\n",
    "    X_val_t   = torch.tensor(X_val_scaled,  dtype=torch.float32, device=device)\n",
    "    y_val_t   = torch.tensor(y_val,         dtype=torch.float32, device=device)\n",
    "\n",
    "    train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "    train_ld = DataLoader(train_ds, batch_size=512, shuffle=True)\n",
    "\n",
    "    model = MyModel().to(device)\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    best_val = float('inf')\n",
    "    patience = 10\n",
    "    counter = 0\n",
    "    max_epochs = 500\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_ld:\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(xb), yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_preds = model(X_val_t)\n",
    "            val_loss = criterion(val_preds, y_val_t).item()\n",
    "\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f\"Early stop at epoch {epoch} (sim {test_id})\")\n",
    "                break\n",
    "\n",
    "    # ------- Save validation preds -------\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_out = model(X_val_t).cpu().numpy().flatten()\n",
    "    pd.DataFrame({'pred': val_out, 'true': y_val.flatten()}).to_csv(\n",
    "        RESULTS_DIR / f'validation_{test_id}.csv', index=False\n",
    "    )\n",
    "\n",
    "    # ------- Yield response function (NO EONR) + TRUE yield (Quadratic-Plateau) -------\n",
    "    test_df = data_2nd_stage[data_2nd_stage['sim'] == test_id].reset_index(drop=True)\n",
    "\n",
    "    # ensure we have parameters for true curve\n",
    "    required_true_cols = ['b0','b1','b2','Nk']\n",
    "    missing = [c for c in required_true_cols if c not in test_df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns for true yield (QP): {missing}\")\n",
    "\n",
    "    # use these for repeating features (keep plateau for scaler input)\n",
    "    base_feats = test_df[['Nk', 'plateau', 'b0', 'b1', 'b2']].reset_index(drop=True)\n",
    "\n",
    "    # N sequence for this sim\n",
    "    eval_seq = evall_N_seq[evall_N_seq['sim'] == test_id].reset_index(drop=True)\n",
    "    if eval_seq.empty:\n",
    "        (RESULTS_DIR / f'yield_response_{test_id}_EMPTY_EVAL_SEQ.csv').write_text(\n",
    "            \"No eval N sequence found for this sim\\n\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    Nseq = eval_seq['N'].to_numpy()\n",
    "    L = len(Nseq)\n",
    "\n",
    "    #  identifiers to carry through\n",
    "    id_cols = [c for c in ['aunit_id', 'cell_id', 'field_id'] if c in test_df.columns]\n",
    "\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(base_feats)):\n",
    "            base = base_feats.iloc[[i]][['Nk','plateau','b0']]  # features used by the ANN\n",
    "            repeated = pd.concat([base] * L, ignore_index=True)\n",
    "            full_feat = pd.concat([repeated, eval_seq[['N']]], axis=1)\n",
    "            full_feat = full_feat[['Nk', 'plateau', 'b0', 'N']]\n",
    "\n",
    "            # model predictions\n",
    "            X_feat = torch.tensor(scaler.transform(full_feat), dtype=torch.float32, device=device)\n",
    "            y_hat  = model(X_feat).cpu().numpy().reshape(-1)\n",
    "\n",
    "            # TRUE yield via Quadratic-Plateau:\n",
    "            # y(N) = b0 + b1*N + b2*N^2  (for N < Nk), else y = b0 + b1*Nk + b2*Nk^2\n",
    "            b0_i = float(base_feats.loc[i, 'b0'])\n",
    "            b1_i = float(base_feats.loc[i, 'b1'])\n",
    "            b2_i = float(base_feats.loc[i, 'b2'])\n",
    "            Nk_i = float(base_feats.loc[i, 'Nk'])\n",
    "\n",
    "            quad_vals    = b0_i + b1_i * Nseq + b2_i * (Nseq**2)\n",
    "            plateau_val  = b0_i + b1_i * Nk_i + b2_i * (Nk_i**2)\n",
    "            true_y       = np.where(Nseq < Nk_i, quad_vals, plateau_val)\n",
    "\n",
    "            row = {\n",
    "                'sim':         [test_id] * L,\n",
    "                'row_id':      [i] * L,\n",
    "                'N':           Nseq,\n",
    "                'pred_yield':  y_hat,\n",
    "                'true_yield':  true_y,   # NEW column\n",
    "            }\n",
    "            # keep N_tilde if present in eval_seq\n",
    "            if 'N_tilde' in eval_seq.columns:\n",
    "                row['N_tilde'] = eval_seq['N_tilde'].to_numpy()\n",
    "            # carry IDs\n",
    "            for col in id_cols:\n",
    "                row[col] = [test_df.loc[i, col]] * L\n",
    "\n",
    "            all_preds.append(pd.DataFrame(row))\n",
    "\n",
    "    df_out = pd.concat(all_preds, ignore_index=True)\n",
    "    df_out.to_csv(RESULTS_DIR / f'yield_response_{test_id}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0099ff",
   "metadata": {},
   "source": [
    "# NO_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "381d5c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test_id: 100%|██████████| 1/1 [04:37<00:00, 277.07s/it]\n"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyreadr\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RepeatedKFold, GridSearchCV\n",
    "\n",
    "# =========================\n",
    "# Project root \n",
    "# =========================\n",
    "def detect_project_root():\n",
    "    env = os.environ.get(\"CF_CONT_ROOT\")\n",
    "    if env:\n",
    "        return Path(env).expanduser().resolve()\n",
    "    try:\n",
    "        here = Path(__file__).resolve()\n",
    "    except NameError:  # e.g., notebooks\n",
    "        here = Path.cwd().resolve()\n",
    "    for p in [here] + list(here.parents):\n",
    "        if p.name == \"CF_Continuous\":\n",
    "            return p\n",
    "        if (p / \"data\" / \"data_20230504\").exists() and (p / \"codes\").exists():\n",
    "            return p\n",
    "    return here\n",
    "\n",
    "PROJ_ROOT = detect_project_root()\n",
    "\n",
    "# =========================\n",
    "# Paths \n",
    "# =========================\n",
    "SPLIT_DIR          = PROJ_ROOT / \"data\" / \"train_test_split\"\n",
    "DATA_2ND_STAGE_RDS = PROJ_ROOT / \"data\" / \"data_20230504\" / \"data_2nd_stage.rds\"\n",
    "EVALL_N_SEQ_RDS    = PROJ_ROOT / \"data\" / \"data_20230504\" / \"evall_N_seq.rds\"\n",
    "\n",
    "# Output \n",
    "model_tag   = \"NO_RF\"\n",
    "n_fields    = 20               # choose 1, 5, or 10\n",
    "run_test_id = 1                # run only this test_id\n",
    "fields_label_map = {1: \"one_field\", 5: \"five_fields\", 10: \"ten_fields\"}\n",
    "fields_label = fields_label_map.get(n_fields, f\"{n_fields}_fields\")\n",
    "\n",
    "RESULTS_BASE = PROJ_ROOT / \"results\" / \"yield_response_function_for_one_iteration\"\n",
    "RESULTS_DIR  = RESULTS_BASE / f\"YRF_{model_tag}_{fields_label}\"\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# === Load data from .rds ===\n",
    "data_2nd_stage = next(iter(pyreadr.read_r(str(DATA_2ND_STAGE_RDS)).values()))\n",
    "evall_N_seq    = next(iter(pyreadr.read_r(str(EVALL_N_SEQ_RDS)).values()))\n",
    "\n",
    "# Make numerics (add b1, b2 so we can compute true_yield)\n",
    "for c in ['yield', 'Nk', 'plateau', 'b0', 'b1', 'b2', 'N']:\n",
    "    if c in data_2nd_stage.columns:\n",
    "        data_2nd_stage[c] = pd.to_numeric(data_2nd_stage[c], errors='coerce')\n",
    "for c in ['sim', 'N', 'N_tilde']:\n",
    "    if c in evall_N_seq.columns:\n",
    "        evall_N_seq[c] = pd.to_numeric(evall_N_seq[c], errors='coerce')\n",
    "\n",
    "# Drop rows with NaNs in key columns for training\n",
    "data_2nd_stage = data_2nd_stage.dropna(subset=['yield','Nk','plateau','b0','N']).reset_index(drop=True)\n",
    "evall_N_seq    = evall_N_seq.dropna(subset=['N']).reset_index(drop=True)\n",
    "\n",
    "# === Load split CSV and restrict to one test_id ===\n",
    "split_csv_path = SPLIT_DIR / f\"train_test_splits_{n_fields}fields.csv\"\n",
    "splits_df = pd.read_csv(split_csv_path)\n",
    "splits_df = splits_df[splits_df['test_id'] == run_test_id].iloc[:1].copy()\n",
    "\n",
    "# === Features for training ===\n",
    "feature_cols = ['Nk', 'plateau', 'b0', 'N']\n",
    "\n",
    "# === Loop (only one iteration) ===\n",
    "for _, row in tqdm(splits_df.iterrows(), total=len(splits_df), desc=\"Processing test_id\"):\n",
    "    test_id = int(row['test_id'])\n",
    "    train_ids = row[[c for c in row.index if c.startswith('train_')]].values\n",
    "\n",
    "    # -------------------------\n",
    "    # train/val data\n",
    "    # -------------------------\n",
    "    dataset = data_2nd_stage[data_2nd_stage['sim'].isin(train_ids)].reset_index(drop=True)\n",
    "    dataset = dataset[['yield'] + feature_cols].copy()\n",
    "\n",
    "    # Random 80/20 split\n",
    "    train_df = dataset.sample(frac=0.8, random_state=0)\n",
    "    val_df   = dataset.drop(train_df.index)\n",
    "\n",
    "    X_train = train_df.drop('yield', axis=1)\n",
    "    y_train = train_df['yield']\n",
    "    X_val   = val_df.drop('yield', axis=1)\n",
    "    y_val   = val_df['yield']\n",
    "\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_val_scaled   = scaler.transform(X_val)\n",
    "\n",
    "    # -------------------------\n",
    "    # Random Forest with CV\n",
    "    # -------------------------\n",
    "    param_grid = {\n",
    "        'max_depth':    [3, 5],\n",
    "        'n_estimators': [50, 250, 500, 1000],\n",
    "        'max_features': [1, 2, 3],\n",
    "    }\n",
    "    rf = RandomForestRegressor(random_state=777)\n",
    "    cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=777)\n",
    "    grid = GridSearchCV(rf, param_grid, cv=cv, n_jobs=-1, scoring='neg_mean_squared_error')\n",
    "    grid.fit(X_train_scaled, y_train)\n",
    "    model = grid.best_estimator_\n",
    "\n",
    "    # -------------------------\n",
    "    # Save validation predictions\n",
    "    # -------------------------\n",
    "    val_preds = model.predict(X_val_scaled)\n",
    "    pd.DataFrame({'pred': val_preds, 'true': y_val.values}).to_csv(\n",
    "        RESULTS_DIR / f'validation_{test_id}.csv', index=False\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # Yield response function (no EONR) + TRUE yield (Quadratic-Plateau)\n",
    "    # -------------------------\n",
    "    test_df  = data_2nd_stage[data_2nd_stage['sim'] == test_id].reset_index(drop=True)\n",
    "\n",
    "    # Ensure parameters are available for the true curve\n",
    "    req_true = ['b0','b1','b2','Nk']\n",
    "    missing  = [c for c in req_true if c not in test_df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns for true yield (QP): {missing}\")\n",
    "\n",
    "    features = test_df[['Nk', 'plateau', 'b0', 'b1', 'b2']].reset_index(drop=True)\n",
    "\n",
    "    # N sequence for THIS test_id\n",
    "    eval_seq = evall_N_seq[evall_N_seq['sim'] == test_id].reset_index(drop=True)\n",
    "    if eval_seq.empty:\n",
    "        (RESULTS_DIR / f'yield_response_{test_id}_EMPTY_EVAL_SEQ.csv').write_text(\n",
    "            \"No eval N sequence found for this sim\\n\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    Nseq = eval_seq['N'].to_numpy()\n",
    "    L = len(Nseq)\n",
    "\n",
    "    #  IDs to carry through\n",
    "    id_cols = [c for c in ['aunit_id', 'cell_id', 'field_id'] if c in test_df.columns]\n",
    "\n",
    "    all_preds = []\n",
    "    for i in range(len(features)):\n",
    "        base = features.iloc[[i]][['Nk','plateau','b0']]          # features used by RF\n",
    "        repeated = pd.concat([base] * L, ignore_index=True)\n",
    "        full_feat = pd.concat([repeated, eval_seq[['N']]], axis=1)\n",
    "        full_feat = full_feat[['Nk', 'plateau', 'b0', 'N']]\n",
    "\n",
    "        # model predictions\n",
    "        X_feat = scaler.transform(full_feat)\n",
    "        preds  = model.predict(X_feat)\n",
    "\n",
    "        # TRUE yield via quadratic-plateau:\n",
    "        # y(N) = b0 + b1*N + b2*N^2  if N < Nk\n",
    "        #      = b0 + b1*Nk + b2*Nk^2 otherwise\n",
    "        b0_i = float(features.loc[i, 'b0'])\n",
    "        b1_i = float(features.loc[i, 'b1'])\n",
    "        b2_i = float(features.loc[i, 'b2'])\n",
    "        Nk_i = float(features.loc[i, 'Nk'])\n",
    "\n",
    "        quad_vals   = b0_i + b1_i * Nseq + b2_i * (Nseq**2)\n",
    "        plateau_val = b0_i + b1_i * Nk_i + b2_i * (Nk_i**2)\n",
    "        true_y      = np.where(Nseq < Nk_i, quad_vals, plateau_val)\n",
    "\n",
    "        row_dict = {\n",
    "            'sim':         [test_id] * L,\n",
    "            'row_id':      [i] * L,\n",
    "            'N':           Nseq,\n",
    "            'pred_yield':  preds,\n",
    "            'true_yield':  true_y\n",
    "        }\n",
    "        # keep N_tilde if present (useful for other models)\n",
    "        if 'N_tilde' in eval_seq.columns:\n",
    "            row_dict['N_tilde'] = eval_seq['N_tilde'].to_numpy()\n",
    "        for col in id_cols:\n",
    "            row_dict[col] = [test_df.loc[i, col]] * L\n",
    "\n",
    "        all_preds.append(pd.DataFrame(row_dict))\n",
    "\n",
    "    df_out = pd.concat(all_preds, ignore_index=True)\n",
    "    df_out.to_csv(RESULTS_DIR / f'yield_response_{test_id}.csv', index=False)\n",
    "\n",
    "# --- Note ---\n",
    "# If test_df has 1,440 rows and N sequence has 100 values → output CSV has 144,000 rows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50ff92b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6462839",
   "metadata": {},
   "source": [
    "# SO_ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e48c2e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test_id:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stop at epoch 78 (sim 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test_id: 100%|██████████| 1/1 [00:09<00:00,  9.13s/it]\n"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyreadr\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# =========================\n",
    "# Project root \n",
    "# =========================\n",
    "def detect_project_root():\n",
    "    env = os.environ.get(\"CF_CONT_ROOT\")\n",
    "    if env:\n",
    "        return Path(env).expanduser().resolve()\n",
    "    try:\n",
    "        here = Path(__file__).resolve()\n",
    "    except NameError:\n",
    "        here = Path.cwd().resolve()\n",
    "    for p in [here] + list(here.parents):\n",
    "        if p.name == \"CF_Continuous\":\n",
    "            return p\n",
    "        if (p / \"data\" / \"data_20230504\").exists() and (p / \"codes\").exists():\n",
    "            return p\n",
    "    return here\n",
    "\n",
    "PROJ_ROOT = detect_project_root()\n",
    "\n",
    "# =========================\n",
    "# Paths\n",
    "# =========================\n",
    "SPLIT_DIR = PROJ_ROOT / \"data\" / \"train_test_split\"\n",
    "DATA_2ND_STAGE_RDS = PROJ_ROOT / \"data\" / \"data_20230504\" / \"data_2nd_stage.rds\"\n",
    "EVALL_N_SEQ_RDS    = PROJ_ROOT / \"data\" / \"data_20230504\" / \"evall_N_seq.rds\"\n",
    "\n",
    "# =========================\n",
    "# Output \n",
    "# =========================\n",
    "model_tag   = \"SO_ANN\"\n",
    "n_fields    = 20              # choose 1, 5, or 10\n",
    "run_test_id = 1                # run only this test_id\n",
    "fields_label_map = {1: \"one_field\", 5: \"five_fields\", 10: \"ten_fields\"}\n",
    "fields_label = fields_label_map.get(n_fields, f\"{n_fields}_fields\")\n",
    "\n",
    "RESULTS_BASE = PROJ_ROOT / \"results\" / \"yield_response_function_for_one_iteration\"\n",
    "RESULTS_DIR  = RESULTS_BASE / f\"YRF_{model_tag}_{fields_label}\"\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === Model  ===\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(4, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# === Load data from .rds ===\n",
    "data_2nd_stage = next(iter(pyreadr.read_r(str(DATA_2ND_STAGE_RDS)).values()))\n",
    "evall_N_seq    = next(iter(pyreadr.read_r(str(EVALL_N_SEQ_RDS)).values()))\n",
    "\n",
    "# Make numerics (include b1, b2 to compute true_yield)\n",
    "for c in ['y_tilde', 'Nk', 'plateau', 'b0', 'b1', 'b2', 'N']:\n",
    "    if c in data_2nd_stage.columns:\n",
    "        data_2nd_stage[c] = pd.to_numeric(data_2nd_stage[c], errors='coerce')\n",
    "for c in ['sim', 'N', 'N_tilde']:\n",
    "    if c in evall_N_seq.columns:\n",
    "        evall_N_seq[c] = pd.to_numeric(evall_N_seq[c], errors='coerce')\n",
    "\n",
    "# Drop rows with NaNs for training\n",
    "data_2nd_stage = data_2nd_stage.dropna(subset=['y_tilde','Nk','plateau','b0','N']).reset_index(drop=True)\n",
    "evall_N_seq    = evall_N_seq.dropna(subset=['N']).reset_index(drop=True)\n",
    "\n",
    "# === Load split CSV and restrict to one test_id ===\n",
    "split_csv_path = SPLIT_DIR / f\"train_test_splits_{n_fields}fields.csv\"\n",
    "splits_df = pd.read_csv(split_csv_path)\n",
    "splits_df = splits_df[splits_df['test_id'] == run_test_id].iloc[:1].copy()\n",
    "\n",
    "# === Features for training ===\n",
    "feature_cols = ['Nk', 'plateau', 'b0', 'N']\n",
    "\n",
    "# === Loop (only one iteration) ===\n",
    "for _, row in tqdm(splits_df.iterrows(), total=len(splits_df), desc=\"Processing test_id\"):\n",
    "    test_id = int(row['test_id'])\n",
    "    train_ids = row[[c for c in row.index if c.startswith('train_')]].values\n",
    "\n",
    "    # -------------------------\n",
    "    # train/val data\n",
    "    # -------------------------\n",
    "    dataset = data_2nd_stage[data_2nd_stage['sim'].isin(train_ids)].reset_index(drop=True)\n",
    "    dataset = dataset[['y_tilde'] + feature_cols].copy()\n",
    "\n",
    "    # Random 80/20 split\n",
    "    train_df = dataset.sample(frac=0.8, random_state=0)\n",
    "    val_df   = dataset.drop(train_df.index)\n",
    "\n",
    "    X_train = train_df.drop('y_tilde', axis=1)\n",
    "    y_train = train_df['y_tilde'].to_numpy().reshape(-1, 1)\n",
    "    X_val   = val_df.drop('y_tilde', axis=1)\n",
    "    y_val   = val_df['y_tilde'].to_numpy().reshape(-1, 1)\n",
    "\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_val_scaled   = scaler.transform(X_val)\n",
    "\n",
    "    X_train_t = torch.tensor(X_train_scaled, dtype=torch.float32, device=device)\n",
    "    y_train_t = torch.tensor(y_train,       dtype=torch.float32, device=device)\n",
    "    X_val_t   = torch.tensor(X_val_scaled,  dtype=torch.float32, device=device)\n",
    "    y_val_t   = torch.tensor(y_val,         dtype=torch.float32, device=device)\n",
    "\n",
    "    train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "    train_ld = DataLoader(train_ds, batch_size=512, shuffle=True)\n",
    "\n",
    "    model = MyModel().to(device)\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    best_val = float('inf')\n",
    "    patience = 10\n",
    "    counter = 0\n",
    "    max_epochs = 500\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_ld:\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_preds = model(X_val_t)\n",
    "            val_loss = criterion(val_preds, y_val_t).item()\n",
    "\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f\"Early stop at epoch {epoch} (sim {test_id})\")\n",
    "                break\n",
    "\n",
    "    # -------------------------\n",
    "    # Save validation predictions\n",
    "    # -------------------------\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_out = model(X_val_t).cpu().numpy().flatten()\n",
    "    pd.DataFrame({'pred': val_out, 'true': y_val.flatten()}).to_csv(\n",
    "        RESULTS_DIR / f'validation_{test_id}.csv', index=False\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # Yield response function (NO EONR) + TRUE yield (Quadratic-Plateau)\n",
    "    # -------------------------\n",
    "    test_df    = data_2nd_stage[data_2nd_stage['sim'] == test_id].reset_index(drop=True)\n",
    "    base_feats = test_df[['Nk', 'plateau', 'b0']].reset_index(drop=True)\n",
    "\n",
    "    # Ensure parameters exist for TRUE curve\n",
    "    req_true = ['b0','b1','b2','Nk']\n",
    "    missing  = [c for c in req_true if c not in test_df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns for true yield (QP): {missing}\")\n",
    "\n",
    "    # N sequence for THIS test_id\n",
    "    eval_seq = evall_N_seq[evall_N_seq['sim'] == test_id].reset_index(drop=True)\n",
    "    if eval_seq.empty:\n",
    "        (RESULTS_DIR / f'yield_response_{test_id}_EMPTY_EVAL_SEQ.csv').write_text(\n",
    "            \"No eval N sequence found for this sim\\n\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    Nseq = eval_seq['N'].to_numpy()\n",
    "    L = len(Nseq)\n",
    "\n",
    "    # identifiers \n",
    "    id_cols = [c for c in ['aunit_id', 'cell_id', 'field_id'] if c in test_df.columns]\n",
    "\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(base_feats)):\n",
    "            base = base_feats.iloc[[i]]                              # 1×3 (Nk, plateau, b0)\n",
    "            repeated = pd.concat([base] * L, ignore_index=True)      # L×3\n",
    "            full_feat = pd.concat([repeated, eval_seq[['N']]], axis=1)  # L×4\n",
    "            full_feat = full_feat[['Nk', 'plateau', 'b0', 'N']]      \n",
    "\n",
    "            X_feat = torch.tensor(scaler.transform(full_feat), dtype=torch.float32, device=device)\n",
    "            y_hat  = model(X_feat).cpu().numpy().reshape(-1)\n",
    "\n",
    "            # --- TRUE yield via quadratic-plateau ---\n",
    "            b0_i = float(test_df.loc[i, 'b0'])\n",
    "            b1_i = float(test_df.loc[i, 'b1'])\n",
    "            b2_i = float(test_df.loc[i, 'b2'])\n",
    "            Nk_i = float(test_df.loc[i, 'Nk'])\n",
    "\n",
    "            quad_vals   = b0_i + b1_i * Nseq + b2_i * (Nseq**2)\n",
    "            plateau_val = b0_i + b1_i * Nk_i + b2_i * (Nk_i**2)\n",
    "            true_y      = np.where(Nseq < Nk_i, quad_vals, plateau_val)\n",
    "\n",
    "            row_dict = {\n",
    "                'sim':         [test_id] * L,\n",
    "                'row_id':      [i] * L,\n",
    "                'N':           Nseq,\n",
    "                'pred_yield':  y_hat,\n",
    "                'true_yield':  true_y\n",
    "            }\n",
    "            # keep N_tilde if present (can be handy for scaled-N models)\n",
    "            if 'N_tilde' in eval_seq.columns:\n",
    "                row_dict['N_tilde'] = eval_seq['N_tilde'].to_numpy()\n",
    "            for col in id_cols:\n",
    "                row_dict[col] = [test_df.loc[i, col]] * L\n",
    "\n",
    "            all_preds.append(pd.DataFrame(row_dict))\n",
    "\n",
    "    df_out = pd.concat(all_preds, ignore_index=True)\n",
    "    df_out.to_csv(RESULTS_DIR / f'yield_response_{test_id}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a539c7",
   "metadata": {},
   "source": [
    "# DO_ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17d6865c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test_id:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stop at epoch 172 (sim 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test_id: 100%|██████████| 1/1 [00:06<00:00,  6.22s/it]\n"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyreadr\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# =========================\n",
    "# Project root \n",
    "# =========================\n",
    "def detect_project_root():\n",
    "    env = os.environ.get(\"CF_CONT_ROOT\")\n",
    "    if env:\n",
    "        return Path(env).expanduser().resolve()\n",
    "    try:\n",
    "        here = Path(__file__).resolve()\n",
    "    except NameError:\n",
    "        here = Path.cwd().resolve()\n",
    "    for p in [here] + list(here.parents):\n",
    "        if p.name == \"CF_Continuous\":\n",
    "            return p\n",
    "        if (p / \"data\" / \"data_20230504\").exists() and (p / \"codes\").exists():\n",
    "            return p\n",
    "    return here\n",
    "\n",
    "PROJ_ROOT = detect_project_root()\n",
    "\n",
    "# =========================\n",
    "# Paths \n",
    "# =========================\n",
    "SPLIT_DIR = PROJ_ROOT / \"data\" / \"train_test_split\"\n",
    "DATA_2ND_STAGE_RDS = PROJ_ROOT / \"data\" / \"data_20230504\" / \"data_2nd_stage.rds\"\n",
    "EVALL_N_SEQ_RDS    = PROJ_ROOT / \"data\" / \"data_20230504\" / \"evall_N_seq.rds\"\n",
    "\n",
    "# =========================\n",
    "# Output \n",
    "# =========================\n",
    "model_tag   = \"DO_ANN\"\n",
    "n_fields    = 20                 # choose 1, 5, or 10\n",
    "run_test_id = 1                 # run only this test_id\n",
    "fields_label_map = {1: \"one_field\", 5: \"five_fields\", 10: \"ten_fields\"}\n",
    "fields_label = fields_label_map.get(n_fields, f\"{n_fields}_fields\")\n",
    "\n",
    "RESULTS_BASE = PROJ_ROOT / \"results\" / \"yield_response_function_for_one_iteration\"\n",
    "RESULTS_DIR  = RESULTS_BASE / f\"YRF_{model_tag}_{fields_label}\"\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === Model  ===\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.branch = nn.Sequential(\n",
    "            nn.Linear(3, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    def forward(self, inp):\n",
    "        x = inp[:, :3]   # Nk, plateau, b0\n",
    "        n = inp[:, 3:4]  # N_tilde (scaled)\n",
    "        return self.branch(x) * n\n",
    "\n",
    "# === Load data from .rds ===\n",
    "data_2nd_stage = next(iter(pyreadr.read_r(str(DATA_2ND_STAGE_RDS)).values()))\n",
    "evall_N_seq    = next(iter(pyreadr.read_r(str(EVALL_N_SEQ_RDS)).values()))\n",
    "\n",
    "# Make numerics (include b1, b2 so we can compute true_yield)\n",
    "for c in ['y_tilde', 'Nk', 'plateau', 'b0', 'b1', 'b2', 'N_tilde', 'N']:\n",
    "    if c in data_2nd_stage.columns:\n",
    "        data_2nd_stage[c] = pd.to_numeric(data_2nd_stage[c], errors='coerce')\n",
    "for c in ['sim', 'N_tilde', 'N']:\n",
    "    if c in evall_N_seq.columns:\n",
    "        evall_N_seq[c] = pd.to_numeric(evall_N_seq[c], errors='coerce')\n",
    "\n",
    "# Clean\n",
    "data_2nd_stage = data_2nd_stage.dropna(subset=['y_tilde','Nk','plateau','b0','N_tilde']).reset_index(drop=True)\n",
    "# Need at least N_tilde in eval sequence\n",
    "need_eval_cols = ['sim', 'N_tilde']\n",
    "if not set(need_eval_cols).issubset(evall_N_seq.columns):\n",
    "    raise ValueError(\"evall_N_seq must contain columns: 'sim' and 'N_tilde'\")\n",
    "evall_N_seq = evall_N_seq.dropna(subset=['N_tilde']).reset_index(drop=True)\n",
    "\n",
    "# === Load split CSV and restrict to one test_id ===\n",
    "split_csv_path = SPLIT_DIR / f\"train_test_splits_{n_fields}fields.csv\"\n",
    "splits_df = pd.read_csv(split_csv_path)\n",
    "splits_df = splits_df[splits_df['test_id'] == run_test_id].iloc[:1].copy()\n",
    "\n",
    "# === Features for training ===\n",
    "feature_cols = ['Nk', 'plateau', 'b0', 'N_tilde']\n",
    "\n",
    "# === Loop (only one iteration) ===\n",
    "for _, row in tqdm(splits_df.iterrows(), total=len(splits_df), desc=\"Processing test_id\"):\n",
    "    test_id = int(row['test_id'])\n",
    "    train_ids = row[[c for c in row.index if c.startswith('train_')]].values\n",
    "\n",
    "    # -------------------------\n",
    "    # train/val data\n",
    "    # -------------------------\n",
    "    dataset = data_2nd_stage[data_2nd_stage['sim'].isin(train_ids)].reset_index(drop=True)\n",
    "    dataset = dataset[['y_tilde'] + feature_cols].copy()\n",
    "\n",
    "    # Random 80/20 split (reproducible)\n",
    "    train_df = dataset.sample(frac=0.8, random_state=0)\n",
    "    val_df   = dataset.drop(train_df.index)\n",
    "\n",
    "    X_train = train_df[feature_cols]\n",
    "    y_train = train_df['y_tilde'].to_numpy().reshape(-1, 1)\n",
    "    X_val   = val_df[feature_cols]\n",
    "    y_val   = val_df['y_tilde'].to_numpy().reshape(-1, 1)\n",
    "\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_val_scaled   = scaler.transform(X_val)\n",
    "\n",
    "    X_train_t = torch.tensor(X_train_scaled, dtype=torch.float32, device=device)\n",
    "    y_train_t = torch.tensor(y_train,       dtype=torch.float32, device=device)\n",
    "    X_val_t   = torch.tensor(X_val_scaled,  dtype=torch.float32, device=device)\n",
    "    y_val_t   = torch.tensor(y_val,         dtype=torch.float32, device=device)\n",
    "\n",
    "    model = MyModel().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.L1Loss()\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 10\n",
    "    counter = 0\n",
    "    max_epochs = 500\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        # manual mini-batch\n",
    "        bs = 512\n",
    "        for i in range(0, len(X_train_t), bs):\n",
    "            xb = X_train_t[i:i+bs]\n",
    "            yb = y_train_t[i:i+bs]\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(xb), yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = criterion(model(X_val_t), y_val_t).item()\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f\"Early stop at epoch {epoch} (sim {test_id})\")\n",
    "                break\n",
    "\n",
    "    # -------------------------\n",
    "    # Save validation predictions\n",
    "    # -------------------------\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_preds = model(X_val_t).cpu().numpy().flatten()\n",
    "    pd.DataFrame({'pred': val_preds, 'true': y_val.flatten()}).to_csv(\n",
    "        RESULTS_DIR / f'validation_{test_id}.csv', index=False\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # Yield response function (NO EONR) + TRUE yield (Quadratic-Plateau)\n",
    "    # -------------------------\n",
    "    test_df   = data_2nd_stage[data_2nd_stage['sim'] == test_id].reset_index(drop=True)\n",
    "    base_feats = test_df[['Nk', 'plateau', 'b0']].reset_index(drop=True)\n",
    "\n",
    "    # Ensure parameters exist for TRUE curve\n",
    "    req_true = ['b0','b1','b2','Nk']\n",
    "    missing  = [c for c in req_true if c not in test_df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns for true yield (QP): {missing}\")\n",
    "\n",
    "    # N sequence (N_tilde) for THIS test_id\n",
    "    eval_seq = evall_N_seq[evall_N_seq['sim'] == test_id].reset_index(drop=True)\n",
    "    if eval_seq.empty:\n",
    "        (RESULTS_DIR / f'yield_response_{test_id}_EMPTY_EVAL_SEQ.csv').write_text(\n",
    "            \"No eval N_tilde sequence found for this sim\\n\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    Nt_seq = eval_seq['N_tilde'].to_numpy()\n",
    "    # Prefer raw N for the true curve if available; else fall back to N_tilde.\n",
    "    if 'N' in eval_seq.columns and eval_seq['N'].notna().any():\n",
    "        N_for_true = eval_seq['N'].to_numpy()\n",
    "    else:\n",
    "        N_for_true = Nt_seq  # fallback if raw N not provided\n",
    "    L = len(Nt_seq)\n",
    "\n",
    "    # IDs to carry through\n",
    "    id_cols = [c for c in ['aunit_id', 'cell_id', 'field_id'] if c in test_df.columns]\n",
    "\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(base_feats)):\n",
    "            base = base_feats.iloc[[i]]                             # 1×3\n",
    "            repeated = pd.concat([base] * L, ignore_index=True)     # L×3\n",
    "            full_feat = pd.concat([repeated, eval_seq[['N_tilde']]], axis=1)  # L×4\n",
    "            full_feat = full_feat[['Nk', 'plateau', 'b0', 'N_tilde']]         \n",
    "\n",
    "            X_feat = torch.tensor(scaler.transform(full_feat), dtype=torch.float32, device=device)\n",
    "            y_hat  = model(X_feat).cpu().numpy().reshape(-1)\n",
    "\n",
    "            # --- TRUE yield via quadratic-plateau using N_for_true ---\n",
    "            b0_i = float(test_df.loc[i, 'b0'])\n",
    "            b1_i = float(test_df.loc[i, 'b1'])\n",
    "            b2_i = float(test_df.loc[i, 'b2'])\n",
    "            Nk_i = float(test_df.loc[i, 'Nk'])\n",
    "\n",
    "            quad_vals   = b0_i + b1_i * N_for_true + b2_i * (N_for_true**2)\n",
    "            plateau_val = b0_i + b1_i * Nk_i       + b2_i * (Nk_i**2)\n",
    "            true_y      = np.where(N_for_true < Nk_i, quad_vals, plateau_val)\n",
    "\n",
    "            row = {\n",
    "                'sim':         [test_id] * L,\n",
    "                'row_id':      [i] * L,\n",
    "                'N_tilde':     Nt_seq,\n",
    "                'pred_yield':  y_hat,\n",
    "                'true_yield':  true_y\n",
    "            }\n",
    "            if 'N' in eval_seq.columns:\n",
    "                row['N'] = eval_seq['N'].to_numpy()\n",
    "            for col in id_cols:\n",
    "                row[col] = [test_df.loc[i, col]] * L\n",
    "\n",
    "            all_preds.append(pd.DataFrame(row))\n",
    "\n",
    "    df_out = pd.concat(all_preds, ignore_index=True)\n",
    "    df_out.to_csv(RESULTS_DIR / f'yield_response_{test_id}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33e0c9b",
   "metadata": {},
   "source": [
    "# SO_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01c325f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test_id: 100%|██████████| 1/1 [05:04<00:00, 304.23s/it]\n"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyreadr\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RepeatedKFold, GridSearchCV\n",
    "\n",
    "# =========================\n",
    "# Project root \n",
    "# =========================\n",
    "def detect_project_root():\n",
    "    env = os.environ.get(\"CF_CONT_ROOT\")\n",
    "    if env:\n",
    "        return Path(env).expanduser().resolve()\n",
    "    try:\n",
    "        here = Path(__file__).resolve()\n",
    "    except NameError:  # e.g., notebooks\n",
    "        here = Path.cwd().resolve()\n",
    "    for p in [here] + list(here.parents):\n",
    "        if p.name == \"CF_Continuous\":\n",
    "            return p\n",
    "        if (p / \"data\" / \"data_20230504\").exists() and (p / \"codes\").exists():\n",
    "            return p\n",
    "    return here\n",
    "\n",
    "PROJ_ROOT = detect_project_root()\n",
    "\n",
    "# =========================\n",
    "# Paths \n",
    "# =========================\n",
    "SPLIT_DIR = PROJ_ROOT / \"data\" / \"train_test_split\"\n",
    "DATA_2ND_STAGE_RDS = PROJ_ROOT / \"data\" / \"data_20230504\" / \"data_2nd_stage.rds\"\n",
    "EVALL_N_SEQ_RDS    = PROJ_ROOT / \"data\" / \"data_20230504\" / \"evall_N_seq.rds\"\n",
    "\n",
    "# =========================\n",
    "# Output \n",
    "# =========================\n",
    "model_tag   = \"SO_RF\"\n",
    "n_fields    = 20                 # choose 1, 5, or 10\n",
    "run_test_id = 1                 # run only this test_id\n",
    "fields_label_map = {1: \"one_field\", 5: \"five_fields\", 10: \"ten_fields\"}\n",
    "fields_label = fields_label_map.get(n_fields, f\"{n_fields}_fields\")\n",
    "\n",
    "RESULTS_BASE = PROJ_ROOT / \"results\" / \"yield_response_function_for_one_iteration\"\n",
    "RESULTS_DIR  = RESULTS_BASE / f\"YRF_{model_tag}_{fields_label}\"\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# === Load data from .rds ===\n",
    "data_2nd_stage = next(iter(pyreadr.read_r(str(DATA_2ND_STAGE_RDS)).values()))\n",
    "evall_N_seq    = next(iter(pyreadr.read_r(str(EVALL_N_SEQ_RDS)).values()))\n",
    "\n",
    "# Make numerics (include b1, b2 so we can compute true_yield)\n",
    "for c in ['y_tilde','Nk','plateau','b0','b1','b2','N']:\n",
    "    if c in data_2nd_stage.columns:\n",
    "        data_2nd_stage[c] = pd.to_numeric(data_2nd_stage[c], errors='coerce')\n",
    "for c in ['sim','N','N_tilde']:\n",
    "    if c in evall_N_seq.columns:\n",
    "        evall_N_seq[c] = pd.to_numeric(evall_N_seq[c], errors='coerce')\n",
    "\n",
    "# Drop rows with NaNs \n",
    "data_2nd_stage = data_2nd_stage.dropna(subset=['y_tilde','Nk','plateau','b0','N']).reset_index(drop=True)\n",
    "evall_N_seq    = evall_N_seq.dropna(subset=['N']).reset_index(drop=True)\n",
    "\n",
    "# === Load split CSV and restrict to one test_id ===\n",
    "split_csv_path = SPLIT_DIR / f\"train_test_splits_{n_fields}fields.csv\"\n",
    "splits_df = pd.read_csv(split_csv_path)\n",
    "splits_df = splits_df[splits_df['test_id'] == run_test_id].iloc[:1].copy()\n",
    "\n",
    "# === Features for training/prediction ===\n",
    "feature_cols = ['Nk', 'plateau', 'b0', 'N']\n",
    "\n",
    "# === Loop (only one iteration) ===\n",
    "for _, row in tqdm(splits_df.iterrows(), total=len(splits_df), desc=\"Processing test_id\"):\n",
    "    test_id = int(row['test_id'])\n",
    "    train_ids = row[[c for c in row.index if c.startswith('train_')]].values\n",
    "\n",
    "    # -------------------------\n",
    "    # Train / validation data\n",
    "    # -------------------------\n",
    "    dataset = data_2nd_stage[data_2nd_stage['sim'].isin(train_ids)].reset_index(drop=True)\n",
    "    dataset = dataset[['y_tilde'] + feature_cols].copy()\n",
    "\n",
    "    train_df = dataset.sample(frac=0.8, random_state=0)\n",
    "    val_df   = dataset.drop(train_df.index)\n",
    "\n",
    "    X_train = train_df.drop('y_tilde', axis=1)\n",
    "    y_train = train_df['y_tilde']\n",
    "    X_val   = val_df.drop('y_tilde', axis=1)\n",
    "    y_val   = val_df['y_tilde']\n",
    "\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_val_scaled   = scaler.transform(X_val)\n",
    "\n",
    "    # -------------------------\n",
    "    # Random Forest with CV\n",
    "    # -------------------------\n",
    "    param_grid = {\n",
    "        'max_depth':    [3, 5],\n",
    "        'n_estimators': [50, 250, 500, 1000],\n",
    "        'max_features': [1, 2, 3],\n",
    "    }\n",
    "    rf = RandomForestRegressor(random_state=777)\n",
    "    cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=777)\n",
    "    grid = GridSearchCV(rf, param_grid, cv=cv, n_jobs=-1, scoring='neg_mean_squared_error')\n",
    "    grid.fit(X_train_scaled, y_train)\n",
    "    model = grid.best_estimator_\n",
    "\n",
    "    # -------------------------\n",
    "    # Save validation predictions\n",
    "    # -------------------------\n",
    "    val_preds = model.predict(X_val_scaled)\n",
    "    pd.DataFrame({'pred': val_preds, 'true': y_val.values}).to_csv(\n",
    "        RESULTS_DIR / f'validation_{test_id}.csv', index=False\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # Yield response function (NO EONR) + TRUE yield (Quadratic-Plateau)\n",
    "    # -------------------------\n",
    "    test_df    = data_2nd_stage[data_2nd_stage['sim'] == test_id].reset_index(drop=True)\n",
    "    base_feats = test_df[['Nk', 'plateau', 'b0']].reset_index(drop=True)\n",
    "\n",
    "    # Ensure parameters exist for TRUE curve\n",
    "    req_true = ['b0','b1','b2','Nk']\n",
    "    missing  = [c for c in req_true if c not in test_df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns for true yield (QP): {missing}\")\n",
    "\n",
    "    # N sequence for THIS test_id\n",
    "    eval_seq = evall_N_seq[evall_N_seq['sim'] == test_id].reset_index(drop=True)\n",
    "    if eval_seq.empty:\n",
    "        (RESULTS_DIR / f'yield_response_{test_id}_EMPTY_EVAL_SEQ.csv').write_text(\n",
    "            \"No eval N sequence found for this sim\\n\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    Nseq = eval_seq['N'].to_numpy()\n",
    "    L = len(Nseq)\n",
    "\n",
    "    # carry optional identifiers\n",
    "    id_cols = [c for c in ['aunit_id', 'cell_id', 'field_id'] if c in test_df.columns]\n",
    "\n",
    "    all_preds = []\n",
    "    for i in range(len(base_feats)):\n",
    "        base = base_feats.iloc[[i]]                               # 1×3\n",
    "        repeated = pd.concat([base] * L, ignore_index=True)       # L×3\n",
    "        full_feat = pd.concat([repeated, eval_seq[['N']]], axis=1)  # L×4\n",
    "        full_feat = full_feat[['Nk', 'plateau', 'b0', 'N']]      \n",
    "\n",
    "        X_feat = scaler.transform(full_feat)\n",
    "        preds  = model.predict(X_feat)\n",
    "\n",
    "        # --- TRUE yield via quadratic-plateau ---\n",
    "        b0_i = float(test_df.loc[i, 'b0'])\n",
    "        b1_i = float(test_df.loc[i, 'b1'])\n",
    "        b2_i = float(test_df.loc[i, 'b2'])\n",
    "        Nk_i = float(test_df.loc[i, 'Nk'])\n",
    "\n",
    "        quad_vals   = b0_i + b1_i * Nseq + b2_i * (Nseq**2)\n",
    "        plateau_val = b0_i + b1_i * Nk_i + b2_i * (Nk_i**2)\n",
    "        true_y      = np.where(Nseq < Nk_i, quad_vals, plateau_val)\n",
    "\n",
    "        row = {\n",
    "            'sim':         [test_id] * L,\n",
    "            'row_id':      [i] * L,\n",
    "            'N':           Nseq,\n",
    "            'pred_yield':  preds,\n",
    "            'true_yield':  true_y,\n",
    "        }\n",
    "        # keep N_tilde if present (harmless, sometimes available in evall_N_seq)\n",
    "        if 'N_tilde' in eval_seq.columns:\n",
    "            row['N_tilde'] = eval_seq['N_tilde'].to_numpy()\n",
    "        for col in id_cols:\n",
    "            row[col] = [test_df.loc[i, col]] * L\n",
    "\n",
    "        all_preds.append(pd.DataFrame(row))\n",
    "\n",
    "    df_out = pd.concat(all_preds, ignore_index=True)\n",
    "    df_out.to_csv(RESULTS_DIR / f'yield_response_{test_id}.csv', index=False)\n",
    "\n",
    "# --- Notes ---\n",
    "# • Trains on ['Nk','plateau','b0','N'] to predict y_tilde.\n",
    "# • Output rows = (# test rows for sim) × (length of N sequence for sim).\n",
    "# • true_yield computed as quadratic-plateau:\n",
    "#     y = b0 + b1*N + b2*N^2  if N < Nk\n",
    "#     y = b0 + b1*Nk + b2*Nk^2 otherwise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939624e2",
   "metadata": {},
   "source": [
    "# CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a74b3e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# === Causal Forest runner (your wrapper) ===\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(\u001b[38;5;18m__file__\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrun_CF_c\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m run_CF_c_py\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# =============================================================\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Project root detection (same pattern you use)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# =============================================================\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "# === CF Yield Response Function (mirrors SO_ANN YRF writer) ===\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyreadr\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# === Causal Forest runner (your wrapper) ===\n",
    "import sys\n",
    "try:\n",
    "    this_dir = Path(__file__).parent\n",
    "except NameError:\n",
    "    this_dir = Path.cwd()\n",
    "sys.path.append(str(this_dir.parent))\n",
    "from run_CF_c import run_CF_c_py\n",
    "\n",
    "# =============================================================\n",
    "# Project root detection (same pattern you use)\n",
    "# =============================================================\n",
    "def detect_project_root():\n",
    "    env = os.environ.get(\"CF_CONT_ROOT\")\n",
    "    if env:\n",
    "        return Path(env).expanduser().resolve()\n",
    "    try:\n",
    "        here = Path(__file__).resolve()\n",
    "    except NameError:\n",
    "        here = Path.cwd().resolve()\n",
    "    for p in [here] + list(here.parents):\n",
    "        if p.name == \"CF_Continuous\":\n",
    "            return p\n",
    "        if (p / \"data\" / \"data_20230504\").exists() and (p / \"codes\").exists():\n",
    "            return p\n",
    "    return here\n",
    "\n",
    "PROJ_ROOT = detect_project_root()\n",
    "\n",
    "# =============================================================\n",
    "# Paths\n",
    "# =============================================================\n",
    "SPLIT_DIR = PROJ_ROOT / \"data\" / \"train_test_split\"\n",
    "DATA_2ND_STAGE_RDS = PROJ_ROOT / \"data\" / \"data_20230504\" / \"data_2nd_stage.rds\"\n",
    "EVALL_N_SEQ_RDS    = PROJ_ROOT / \"data\" / \"data_20230504\" / \"evall_N_seq.rds\"\n",
    "\n",
    "# =============================================================\n",
    "# Output setup (mirrors SO_ANN layout)\n",
    "# =============================================================\n",
    "model_tag   = \"CF\"\n",
    "n_fields    = 20                 # choose from {1, 3, 5, 10, 20}\n",
    "run_test_id = 1                  # run only this test_id, like your SO_ANN script\n",
    "\n",
    "fields_label_map = {1: \"one_field\", 3: \"three_fields\", 5: \"five_fields\",\n",
    "                    10: \"ten_fields\", 20: \"twenty_fields\"}\n",
    "fields_label = fields_label_map.get(n_fields, f\"{n_fields}_fields\")\n",
    "\n",
    "RESULTS_BASE = PROJ_ROOT / \"results\" / \"yield_response_function_for_one_iteration\"\n",
    "RESULTS_DIR  = RESULTS_BASE / f\"YRF_{model_tag}_{fields_label}\"\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# =============================================================\n",
    "# Economic parameters (for true EONR if needed; not used for YRF file)\n",
    "# =============================================================\n",
    "p_corn = 6.25 / 25.4  # $/kg\n",
    "p_N    = 1 / 0.453592 # $/kg\n",
    "\n",
    "# =============================================================\n",
    "# Spline basis (mgcv::cr equivalent via patsy)\n",
    "# =============================================================\n",
    "from patsy import dmatrix, build_design_matrices, cr\n",
    "k_splines = 4\n",
    "spline_formula = f\"cr(N, df={k_splines}) - 1\"\n",
    "\n",
    "def prepare_T_mat_py(train_df, test_df):\n",
    "    \"\"\"Build train/test spline basis using train support and clip test N to that support.\"\"\"\n",
    "    N_train = train_df[\"N\"].to_numpy()\n",
    "    N_min, N_max = float(np.min(N_train)), float(np.max(N_train))\n",
    "\n",
    "    T_train_df = dmatrix(spline_formula, {\"N\": N_train}, return_type=\"dataframe\")\n",
    "    design_info = T_train_df.design_info\n",
    "\n",
    "    N_test_clip = np.clip(test_df[\"N\"].to_numpy(), N_min, N_max)\n",
    "    T_test_mat = build_design_matrices([design_info], {\"N\": N_test_clip})[0]\n",
    "    T_test_df = pd.DataFrame(np.asarray(T_test_mat), columns=T_train_df.columns)\n",
    "\n",
    "    col_names = [f\"T_{i+1}\" for i in range(T_train_df.shape[1])]\n",
    "    T_train_df.columns = col_names\n",
    "    T_test_df.columns  = col_names\n",
    "\n",
    "    return T_train_df, T_test_df, design_info, (N_min, N_max)\n",
    "\n",
    "# =============================================================\n",
    "# Load data\n",
    "# =============================================================\n",
    "data_2nd_stage = next(iter(pyreadr.read_r(str(DATA_2ND_STAGE_RDS)).values()))\n",
    "evall_N_seq    = next(iter(pyreadr.read_r(str(EVALL_N_SEQ_RDS)).values()))\n",
    "\n",
    "# Ensure numeric types\n",
    "for c in ['yield', 'y_tilde', 'Nk', 'plateau', 'b0', 'b1', 'b2', 'N']:\n",
    "    if c in data_2nd_stage.columns:\n",
    "        data_2nd_stage[c] = pd.to_numeric(data_2nd_stage[c], errors='coerce')\n",
    "for c in ['sim', 'N', 'N_tilde']:\n",
    "    if c in evall_N_seq.columns:\n",
    "        evall_N_seq[c] = pd.to_numeric(evall_N_seq[c], errors='coerce')\n",
    "\n",
    "# Drop rows with NaNs required for training\n",
    "data_2nd_stage = data_2nd_stage.dropna(subset=['yield','Nk','plateau','b0','N']).reset_index(drop=True)\n",
    "evall_N_seq    = evall_N_seq.dropna(subset=['N']).reset_index(drop=True)\n",
    "\n",
    "# =============================================================\n",
    "# Load split CSV; restrict to the chosen test_id like SO_ANN\n",
    "# =============================================================\n",
    "split_csv_path = SPLIT_DIR / f\"train_test_splits_{n_fields}fields.csv\"\n",
    "splits_df = pd.read_csv(split_csv_path)\n",
    "splits_df = splits_df[splits_df['test_id'] == run_test_id].iloc[:1].copy()\n",
    "\n",
    "# Features X (environment; same as your CF EONR code)\n",
    "x_vars = [\"Nk\", \"plateau\", \"b0\"]\n",
    "\n",
    "# =============================================================\n",
    "# Main loop (single test_id to mirror SO_ANN example)\n",
    "# =============================================================\n",
    "for _, row in tqdm(splits_df.iterrows(), total=len(splits_df), desc=\"Processing test_id\"):\n",
    "    test_id   = int(row[\"test_id\"])\n",
    "    train_ids = row[[c for c in row.index if c.startswith(\"train_\")]].dropna().astype(int).tolist()\n",
    "\n",
    "    # Train/test partitions\n",
    "    train_df = data_2nd_stage[data_2nd_stage[\"sim\"].isin(train_ids)].copy()\n",
    "    test_df  = data_2nd_stage[data_2nd_stage[\"sim\"] == test_id].copy()\n",
    "\n",
    "    # Remove any prior T_ columns\n",
    "    train_df = train_df.loc[:, ~train_df.columns.str.startswith(\"T_\")]\n",
    "    test_df  = test_df.loc[:,  ~test_df.columns.str.startswith(\"T_\")]\n",
    "\n",
    "    # Build spline basis on train support\n",
    "    T_train_df, T_test_df, design_info, (N_min_tr, N_max_tr) = prepare_T_mat_py(train_df, test_df)\n",
    "    T_vars   = T_train_df.columns.tolist()\n",
    "\n",
    "    train_df = pd.concat([train_df.reset_index(drop=True), T_train_df], axis=1)\n",
    "    test_df  = pd.concat([test_df.reset_index(drop=True),  T_test_df], axis=1)\n",
    "\n",
    "    # Prepare arrays\n",
    "    Y     = train_df[\"yield\"].to_numpy()\n",
    "    X     = train_df[x_vars].to_numpy()\n",
    "    T_mat = train_df[T_vars].to_numpy()\n",
    "    W     = X  # same as your CF EONR script\n",
    "    X_test = test_df[x_vars].to_numpy()\n",
    "\n",
    "    # Scale X (same as EONR flow)\n",
    "    X_scaler     = StandardScaler().fit(X)\n",
    "    X_scaled     = X_scaler.transform(X)\n",
    "    X_test_scaled= X_scaler.transform(X_test)\n",
    "\n",
    "    # Train CF\n",
    "    te_hat_cf = run_CF_c_py(\n",
    "        Y=Y,\n",
    "        T=T_mat,\n",
    "        X=X_scaled,\n",
    "        W=W,\n",
    "        n_estimators=2000,\n",
    "        min_samples_leaf=20,\n",
    "        random_state=78343\n",
    "    )\n",
    "\n",
    "    # Predict per-unit coefficients (one coefficient per spline basis)\n",
    "    te_hat = te_hat_cf.const_marginal_effect(X_test_scaled)\n",
    "    te_hat = np.atleast_2d(te_hat)   # shape: [n_test_obs, n_T]\n",
    "    n_T    = T_mat.shape[1]\n",
    "    if te_hat.shape[1] != n_T:\n",
    "        # very defensive; usually matches\n",
    "        te_hat = np.resize(te_hat, (te_hat.shape[0], n_T))\n",
    "\n",
    "    # ===== Build evaluation N sequence for THIS sim (mirrors SO_ANN) =====\n",
    "    eval_seq = evall_N_seq[evall_N_seq['sim'] == test_id].reset_index(drop=True)\n",
    "    if eval_seq.empty:\n",
    "        (RESULTS_DIR / f'yield_response_{test_id}_EMPTY_EVAL_SEQ.csv').write_text(\n",
    "            \"No eval N sequence found for this sim\\n\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    Nseq = eval_seq['N'].to_numpy()\n",
    "    # To avoid spline extrapolation, clip to train N support\n",
    "    Nseq_clip = np.clip(Nseq, N_min_tr, N_max_tr)\n",
    "\n",
    "    # Build spline basis for the whole N sequence using the TRAIN design_info\n",
    "    eval_T_mat  = build_design_matrices([design_info], {\"N\": Nseq_clip})[0]\n",
    "    eval_T_full = np.asarray(eval_T_mat)          # shape [L, n_T]\n",
    "    if eval_T_full.shape[1] != n_T:\n",
    "        eval_T_full = eval_T_full[:, :n_T]\n",
    "\n",
    "    # ===== Predicted yield curves: curv[i, :] = te_hat[i] @ eval_T_full.T =====\n",
    "    curv = te_hat @ eval_T_full.T                 # shape [n_test_obs, L]\n",
    "\n",
    "    # ===== True yield (Quadratic-Plateau) =====\n",
    "    req_true = ['b0','b1','b2','Nk']\n",
    "    missing = [c for c in req_true if c not in test_df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns for true yield (QP): {missing}\")\n",
    "\n",
    "    b0_vec = test_df['b0'].to_numpy()\n",
    "    b1_vec = test_df['b1'].to_numpy()\n",
    "    b2_vec = test_df['b2'].to_numpy()\n",
    "    Nk_vec = test_df['Nk'].to_numpy()\n",
    "\n",
    "    # Broadcast to produce [n_test_obs, L]\n",
    "    quad   = b0_vec[:, None] + b1_vec[:, None]*Nseq[None, :] + b2_vec[:, None]*(Nseq[None, :]**2)\n",
    "    plate  = b0_vec[:, None] + b1_vec[:, None]*Nk_vec[:, None] + b2_vec[:, None]*(Nk_vec[:, None]**2)\n",
    "    true_y = np.where(Nseq[None, :] < Nk_vec[:, None], quad, plate)\n",
    "\n",
    "    # ===== Assemble output like SO_ANN =====\n",
    "    id_cols = [c for c in ['aunit_id', 'cell_id', 'field_id'] if c in test_df.columns]\n",
    "    L = len(Nseq)\n",
    "    rows = []\n",
    "    for i in range(test_df.shape[0]):\n",
    "        row_dict = {\n",
    "            'sim':        [test_id] * L,\n",
    "            'row_id':     [i] * L,\n",
    "            'N':          Nseq,\n",
    "            'pred_yield': curv[i, :],\n",
    "            'true_yield': true_y[i, :]\n",
    "        }\n",
    "        if 'N_tilde' in eval_seq.columns:\n",
    "            row_dict['N_tilde'] = eval_seq['N_tilde'].to_numpy()\n",
    "        for col in id_cols:\n",
    "            row_dict[col] = [test_df.loc[i, col]] * L\n",
    "        rows.append(pd.DataFrame(row_dict))\n",
    "\n",
    "    df_out = pd.concat(rows, ignore_index=True)\n",
    "    out_path = RESULTS_DIR / f'yield_response_{test_id}.csv'\n",
    "    df_out.to_csv(out_path, index=False)\n",
    "    print(f\"✅ Wrote {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc59e4d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "086f974e",
   "metadata": {},
   "source": [
    "# NO_ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c294d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test_id:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stop at epoch 336 (sim 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test_id: 100%|██████████| 1/1 [02:26<00:00, 146.01s/it]\n"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyreadr\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# =========================\n",
    "# Project root \n",
    "# =========================\n",
    "def detect_project_root():\n",
    "    env = os.environ.get(\"CF_CONT_ROOT\")\n",
    "    if env:\n",
    "        return Path(env).expanduser().resolve()\n",
    "    try:\n",
    "        here = Path(__file__).resolve()\n",
    "    except NameError:\n",
    "        here = Path.cwd().resolve()\n",
    "    for p in [here] + list(here.parents):\n",
    "        if p.name == \"CF_Continuous\":\n",
    "            return p\n",
    "        if (p / \"data\" / \"data_20230504\").exists() and (p / \"codes\").exists():\n",
    "            return p\n",
    "    return here\n",
    "\n",
    "PROJ_ROOT = detect_project_root()\n",
    "\n",
    "# =========================\n",
    "# Paths \n",
    "# =========================\n",
    "SPLIT_DIR = PROJ_ROOT / \"data\" / \"train_test_split\"\n",
    "DATA_2ND_STAGE_RDS = PROJ_ROOT / \"data\" / \"data_20230504\" / \"data_2nd_stage.rds\"\n",
    "EVALL_N_SEQ_RDS    = PROJ_ROOT / \"data\" / \"data_20230504\" / \"evall_N_seq.rds\"\n",
    "\n",
    "# =========================\n",
    "# Output \n",
    "# =========================\n",
    "model_tag   = \"NO_ANN\"\n",
    "n_fields    = 5                # choose 1, 5, or 10\n",
    "run_test_id = 1                # run only this test_id\n",
    "fields_label_map = {1: \"one_field\", 5: \"five_fields\", 10: \"ten_fields\"}\n",
    "fields_label = fields_label_map.get(n_fields, f\"{n_fields}_fields\")\n",
    "\n",
    "RESULTS_BASE = PROJ_ROOT / \"results\" / \"yield_response_function_for_one_iteration\"\n",
    "RESULTS_DIR  = RESULTS_BASE / f\"YRF_{model_tag}_{fields_label}\"\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === Model: Simple ANN (NO ANN) ===\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(4, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# --- Load data ---\n",
    "data_2nd_stage = next(iter(pyreadr.read_r(str(DATA_2ND_STAGE_RDS)).values()))\n",
    "evall_N_seq    = next(iter(pyreadr.read_r(str(EVALL_N_SEQ_RDS)).values()))\n",
    "\n",
    "# \n",
    "for c in ['yield','Nk','plateau','b0','N']:\n",
    "    if c in data_2nd_stage.columns:\n",
    "        data_2nd_stage[c] = pd.to_numeric(data_2nd_stage[c], errors='coerce')\n",
    "for c in ['sim','N']:\n",
    "    if c in evall_N_seq.columns:\n",
    "        evall_N_seq[c] = pd.to_numeric(evall_N_seq[c], errors='coerce')\n",
    "\n",
    "# Clean\n",
    "data_2nd_stage = data_2nd_stage.dropna(subset=['yield','Nk','plateau','b0','N']).reset_index(drop=True)\n",
    "evall_N_seq    = evall_N_seq.dropna(subset=['N']).reset_index(drop=True)\n",
    "\n",
    "# --- Load split CSV and restrict to one test_id ---\n",
    "split_csv_path = SPLIT_DIR / f\"train_test_splits_{n_fields}fields.csv\"\n",
    "splits_df = pd.read_csv(split_csv_path)\n",
    "splits_df = splits_df[splits_df['test_id'] == run_test_id].iloc[:1].copy()\n",
    "\n",
    "# Features\n",
    "feature_cols = ['Nk', 'plateau', 'b0', 'N']\n",
    "\n",
    "# === Loop (only one iteration) ===\n",
    "for _, row in tqdm(splits_df.iterrows(), total=len(splits_df), desc=\"Processing test_id\"):\n",
    "    test_id = int(row['test_id'])\n",
    "    train_ids = row[[c for c in row.index if c.startswith('train_')]].values\n",
    "\n",
    "    # ------- Train/val -------\n",
    "    dataset = data_2nd_stage[data_2nd_stage['sim'].isin(train_ids)].reset_index(drop=True)\n",
    "    dataset = dataset[['yield'] + feature_cols].copy()\n",
    "\n",
    "    train_df = dataset.sample(frac=0.8, random_state=0)\n",
    "    val_df   = dataset.drop(train_df.index)\n",
    "\n",
    "    X_train = train_df.drop('yield', axis=1)\n",
    "    y_train = train_df['yield'].to_numpy().reshape(-1, 1)\n",
    "    X_val   = val_df.drop('yield', axis=1)\n",
    "    y_val   = val_df['yield'].to_numpy().reshape(-1, 1)\n",
    "\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_val_scaled   = scaler.transform(X_val)\n",
    "\n",
    "    X_train_t = torch.tensor(X_train_scaled, dtype=torch.float32, device=device)\n",
    "    y_train_t = torch.tensor(y_train,       dtype=torch.float32, device=device)\n",
    "    X_val_t   = torch.tensor(X_val_scaled,  dtype=torch.float32, device=device)\n",
    "    y_val_t   = torch.tensor(y_val,         dtype=torch.float32, device=device)\n",
    "\n",
    "    train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "    train_ld = DataLoader(train_ds, batch_size=512, shuffle=True)\n",
    "\n",
    "    model = MyModel().to(device)\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    best_val = float('inf')\n",
    "    patience = 10\n",
    "    counter = 0\n",
    "    max_epochs = 500\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_ld:\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(xb), yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_preds = model(X_val_t)\n",
    "            val_loss = criterion(val_preds, y_val_t).item()\n",
    "\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f\"Early stop at epoch {epoch} (sim {test_id})\")\n",
    "                break\n",
    "\n",
    "    # ------- Save validation preds -------\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_out = model(X_val_t).cpu().numpy().flatten()\n",
    "    pd.DataFrame({'pred': val_out, 'true': y_val.flatten()}).to_csv(\n",
    "        RESULTS_DIR / f'validation_{test_id}.csv', index=False\n",
    "    )\n",
    "\n",
    "    # ------- Yield response function (NO EONR) -------\n",
    "    test_df   = data_2nd_stage[data_2nd_stage['sim'] == test_id].reset_index(drop=True)\n",
    "    base_feats = test_df[['Nk', 'plateau', 'b0']].reset_index(drop=True)\n",
    "\n",
    "    # N sequence for this sim\n",
    "    eval_seq = evall_N_seq[evall_N_seq['sim'] == test_id].reset_index(drop=True)\n",
    "    if eval_seq.empty:\n",
    "        (RESULTS_DIR / f'yield_response_{test_id}_EMPTY_EVAL_SEQ.csv').write_text(\n",
    "            \"No eval N sequence found for this sim\\n\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    Nseq = eval_seq['N'].to_numpy()\n",
    "    L = len(Nseq)\n",
    "\n",
    "    id_cols = [c for c in ['aunit_id', 'cell_id', 'field_id'] if c in test_df.columns]\n",
    "\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(base_feats)):\n",
    "            base = base_feats.iloc[[i]]\n",
    "            repeated = pd.concat([base] * L, ignore_index=True)\n",
    "            full_feat = pd.concat([repeated, eval_seq[['N']]], axis=1)\n",
    "            full_feat = full_feat[['Nk', 'plateau', 'b0', 'N']]\n",
    "\n",
    "            X_feat = torch.tensor(scaler.transform(full_feat), dtype=torch.float32, device=device)\n",
    "            y_hat  = model(X_feat).cpu().numpy().reshape(-1)\n",
    "\n",
    "            row = {\n",
    "                'sim':        [test_id] * L,\n",
    "                'row_id':     [i] * L,\n",
    "                'N':          Nseq,\n",
    "                'pred_yield': y_hat\n",
    "            }\n",
    "            for col in id_cols:\n",
    "                row[col] = [test_df.loc[i, col]] * L\n",
    "\n",
    "            all_preds.append(pd.DataFrame(row))\n",
    "\n",
    "    df_out = pd.concat(all_preds, ignore_index=True)\n",
    "    df_out.to_csv(RESULTS_DIR / f'yield_response_{test_id}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0099ff",
   "metadata": {},
   "source": [
    "# NO_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccd732f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test_id: 100%|██████████| 1/1 [01:12<00:00, 72.74s/it]\n"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyreadr\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RepeatedKFold, GridSearchCV\n",
    "\n",
    "# =========================\n",
    "# Project root \n",
    "# =========================\n",
    "def detect_project_root():\n",
    "    env = os.environ.get(\"CF_CONT_ROOT\")\n",
    "    if env:\n",
    "        return Path(env).expanduser().resolve()\n",
    "    try:\n",
    "        here = Path(__file__).resolve()\n",
    "    except NameError:  # e.g., notebooks\n",
    "        here = Path.cwd().resolve()\n",
    "    for p in [here] + list(here.parents):\n",
    "        if p.name == \"CF_Continuous\":\n",
    "            return p\n",
    "        if (p / \"data\" / \"data_20230504\").exists() and (p / \"codes\").exists():\n",
    "            return p\n",
    "    return here\n",
    "\n",
    "PROJ_ROOT = detect_project_root()\n",
    "\n",
    "# =========================\n",
    "# Paths \n",
    "# =========================\n",
    "SPLIT_DIR          = PROJ_ROOT / \"data\" / \"train_test_split\"\n",
    "DATA_2ND_STAGE_RDS = PROJ_ROOT / \"data\" / \"data_20230504\" / \"data_2nd_stage.rds\"\n",
    "EVALL_N_SEQ_RDS    = PROJ_ROOT / \"data\" / \"data_20230504\" / \"evall_N_seq.rds\"\n",
    "\n",
    "# Output \n",
    "model_tag   = \"NO_RF\"\n",
    "n_fields    = 5                # choose 1, 5, or 10\n",
    "run_test_id = 1                # run only this test_id\n",
    "fields_label_map = {1: \"one_field\", 5: \"five_fields\", 10: \"ten_fields\"}\n",
    "fields_label = fields_label_map.get(n_fields, f\"{n_fields}_fields\")\n",
    "\n",
    "RESULTS_BASE = PROJ_ROOT / \"results\" / \"yield_response_function_for_one_iteration\"\n",
    "RESULTS_DIR  = RESULTS_BASE / f\"YRF_{model_tag}_{fields_label}\"\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# === Load data from .rds ===\n",
    "data_2nd_stage = next(iter(pyreadr.read_r(str(DATA_2ND_STAGE_RDS)).values()))\n",
    "evall_N_seq    = next(iter(pyreadr.read_r(str(EVALL_N_SEQ_RDS)).values()))\n",
    "\n",
    "# \n",
    "for c in ['yield', 'Nk', 'plateau', 'b0', 'N']:\n",
    "    if c in data_2nd_stage.columns:\n",
    "        data_2nd_stage[c] = pd.to_numeric(data_2nd_stage[c], errors='coerce')\n",
    "for c in ['sim', 'N']:\n",
    "    if c in evall_N_seq.columns:\n",
    "        evall_N_seq[c] = pd.to_numeric(evall_N_seq[c], errors='coerce')\n",
    "\n",
    "# Drop rows with NaNs in key columns\n",
    "data_2nd_stage = data_2nd_stage.dropna(subset=['yield','Nk','plateau','b0','N']).reset_index(drop=True)\n",
    "evall_N_seq    = evall_N_seq.dropna(subset=['N']).reset_index(drop=True)\n",
    "\n",
    "# === Load split CSV and restrict to one test_id ===\n",
    "split_csv_path = SPLIT_DIR / f\"train_test_splits_{n_fields}fields.csv\"\n",
    "splits_df = pd.read_csv(split_csv_path)\n",
    "splits_df = splits_df[splits_df['test_id'] == run_test_id].iloc[:1].copy()\n",
    "\n",
    "# === Features for training ===\n",
    "feature_cols = ['Nk', 'plateau', 'b0', 'N']\n",
    "\n",
    "# === Loop (only one iteration) ===\n",
    "for _, row in tqdm(splits_df.iterrows(), total=len(splits_df), desc=\"Processing test_id\"):\n",
    "    test_id = int(row['test_id'])\n",
    "    train_ids = row[[c for c in row.index if c.startswith('train_')]].values\n",
    "\n",
    "    # -------------------------\n",
    "    # train/val data\n",
    "    # -------------------------\n",
    "    dataset = data_2nd_stage[data_2nd_stage['sim'].isin(train_ids)].reset_index(drop=True)\n",
    "    dataset = dataset[['yield'] + feature_cols].copy()\n",
    "\n",
    "    # Random 80/20 split\n",
    "    train_df = dataset.sample(frac=0.8, random_state=0)\n",
    "    val_df   = dataset.drop(train_df.index)\n",
    "\n",
    "    X_train = train_df.drop('yield', axis=1)\n",
    "    y_train = train_df['yield']\n",
    "    X_val   = val_df.drop('yield', axis=1)\n",
    "    y_val   = val_df['yield']\n",
    "\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_val_scaled   = scaler.transform(X_val)\n",
    "\n",
    "    # -------------------------\n",
    "    # Random Forest with CV\n",
    "    # -------------------------\n",
    "    param_grid = {\n",
    "        'max_depth':    [3, 5],\n",
    "        'n_estimators': [50, 250, 500, 1000],\n",
    "        'max_features': [1, 2, 3],\n",
    "    }\n",
    "    rf = RandomForestRegressor(random_state=777)\n",
    "    cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=777)\n",
    "    grid = GridSearchCV(rf, param_grid, cv=cv, n_jobs=-1, scoring='neg_mean_squared_error')\n",
    "    grid.fit(X_train_scaled, y_train)\n",
    "    model = grid.best_estimator_\n",
    "\n",
    "    # -------------------------\n",
    "    # Save validation predictions\n",
    "    # -------------------------\n",
    "    val_preds = model.predict(X_val_scaled)\n",
    "    pd.DataFrame({'pred': val_preds, 'true': y_val.values}).to_csv(\n",
    "        RESULTS_DIR / f'validation_{test_id}.csv', index=False\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # Yield response function (no EONR)\n",
    "    # -------------------------\n",
    "    test_df  = data_2nd_stage[data_2nd_stage['sim'] == test_id].reset_index(drop=True)\n",
    "    features = test_df[['Nk', 'plateau', 'b0']].reset_index(drop=True)\n",
    "\n",
    "    # N sequence for THIS test_id\n",
    "    eval_seq = evall_N_seq[evall_N_seq['sim'] == test_id].reset_index(drop=True)\n",
    "    if eval_seq.empty:\n",
    "        (RESULTS_DIR / f'yield_response_{test_id}_EMPTY_EVAL_SEQ.csv').write_text(\n",
    "            \"No eval N sequence found for this sim\\n\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    Nseq = eval_seq['N'].to_numpy()\n",
    "    L = len(Nseq)\n",
    "\n",
    "    # \n",
    "    id_cols = [c for c in ['aunit_id', 'cell_id', 'field_id'] if c in test_df.columns]\n",
    "\n",
    "    all_preds = []\n",
    "    for i in range(len(features)):\n",
    "        base = features.iloc[[i]]\n",
    "        repeated = pd.concat([base] * L, ignore_index=True)\n",
    "        full_feat = pd.concat([repeated, eval_seq[['N']]], axis=1)\n",
    "        full_feat = full_feat[['Nk', 'plateau', 'b0', 'N']]\n",
    "\n",
    "        X_feat = scaler.transform(full_feat)\n",
    "        preds  = model.predict(X_feat)\n",
    "\n",
    "        row_dict = {\n",
    "            'sim':        [test_id] * L,\n",
    "            'row_id':     [i] * L,\n",
    "            'N':          Nseq,\n",
    "            'pred_yield': preds\n",
    "        }\n",
    "        for col in id_cols:\n",
    "            row_dict[col] = [test_df.loc[i, col]] * L\n",
    "\n",
    "        all_preds.append(pd.DataFrame(row_dict))\n",
    "\n",
    "    df_out = pd.concat(all_preds, ignore_index=True)\n",
    "    df_out.to_csv(RESULTS_DIR / f'yield_response_{test_id}.csv', index=False)\n",
    "\n",
    "# --- Note ---\n",
    "# If test_df has 1,440 rows and N sequence has 100 values → output CSV has 144,000 rows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6462839",
   "metadata": {},
   "source": [
    "# SO_ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a4f1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test_id:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stop at epoch 143 (sim 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test_id: 100%|██████████| 1/1 [02:07<00:00, 127.07s/it]\n"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyreadr\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# =========================\n",
    "# Project root \n",
    "# =========================\n",
    "def detect_project_root():\n",
    "    env = os.environ.get(\"CF_CONT_ROOT\")\n",
    "    if env:\n",
    "        return Path(env).expanduser().resolve()\n",
    "    try:\n",
    "        here = Path(__file__).resolve()\n",
    "    except NameError:\n",
    "        here = Path.cwd().resolve()\n",
    "    for p in [here] + list(here.parents):\n",
    "        if p.name == \"CF_Continuous\":\n",
    "            return p\n",
    "        if (p / \"data\" / \"data_20230504\").exists() and (p / \"codes\").exists():\n",
    "            return p\n",
    "    return here\n",
    "\n",
    "PROJ_ROOT = detect_project_root()\n",
    "\n",
    "# =========================\n",
    "# Paths\n",
    "# =========================\n",
    "SPLIT_DIR = PROJ_ROOT / \"data\" / \"train_test_split\"\n",
    "DATA_2ND_STAGE_RDS = PROJ_ROOT / \"data\" / \"data_20230504\" / \"data_2nd_stage.rds\"\n",
    "EVALL_N_SEQ_RDS    = PROJ_ROOT / \"data\" / \"data_20230504\" / \"evall_N_seq.rds\"\n",
    "\n",
    "# =========================\n",
    "# Output \n",
    "# =========================\n",
    "model_tag   = \"SO_ANN\"\n",
    "n_fields    = 10               # choose 1, 5, or 10\n",
    "run_test_id = 1                # run only this test_id\n",
    "fields_label_map = {1: \"one_field\", 5: \"five_fields\", 10: \"ten_fields\"}\n",
    "fields_label = fields_label_map.get(n_fields, f\"{n_fields}_fields\")\n",
    "\n",
    "RESULTS_BASE = PROJ_ROOT / \"results\" / \"yield_response_function_for_one_iteration\"\n",
    "RESULTS_DIR  = RESULTS_BASE / f\"YRF_{model_tag}_{fields_label}\"\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === Model  ===\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(4, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# === Load data from .rds ===\n",
    "data_2nd_stage = next(iter(pyreadr.read_r(str(DATA_2ND_STAGE_RDS)).values()))\n",
    "evall_N_seq    = next(iter(pyreadr.read_r(str(EVALL_N_SEQ_RDS)).values()))\n",
    "\n",
    "# \n",
    "for c in ['y_tilde', 'Nk', 'plateau', 'b0', 'N']:\n",
    "    if c in data_2nd_stage.columns:\n",
    "        data_2nd_stage[c] = pd.to_numeric(data_2nd_stage[c], errors='coerce')\n",
    "for c in ['sim', 'N']:\n",
    "    if c in evall_N_seq.columns:\n",
    "        evall_N_seq[c] = pd.to_numeric(evall_N_seq[c], errors='coerce')\n",
    "\n",
    "# Drop rows with NaNs \n",
    "data_2nd_stage = data_2nd_stage.dropna(subset=['y_tilde','Nk','plateau','b0','N']).reset_index(drop=True)\n",
    "evall_N_seq    = evall_N_seq.dropna(subset=['N']).reset_index(drop=True)\n",
    "\n",
    "# === Load split CSV and restrict to one test_id ===\n",
    "split_csv_path = SPLIT_DIR / f\"train_test_splits_{n_fields}fields.csv\"\n",
    "splits_df = pd.read_csv(split_csv_path)\n",
    "splits_df = splits_df[splits_df['test_id'] == run_test_id].iloc[:1].copy()\n",
    "\n",
    "# === Features for training ===\n",
    "feature_cols = ['Nk', 'plateau', 'b0', 'N']\n",
    "\n",
    "# === Loop (only one iteration) ===\n",
    "for _, row in tqdm(splits_df.iterrows(), total=len(splits_df), desc=\"Processing test_id\"):\n",
    "    test_id = int(row['test_id'])\n",
    "    train_ids = row[[c for c in row.index if c.startswith('train_')]].values\n",
    "\n",
    "    # -------------------------\n",
    "    # train/val data\n",
    "    # -------------------------\n",
    "    dataset = data_2nd_stage[data_2nd_stage['sim'].isin(train_ids)].reset_index(drop=True)\n",
    "    dataset = dataset[['y_tilde'] + feature_cols].copy()\n",
    "\n",
    "    # Random 80/20 split\n",
    "    train_df = dataset.sample(frac=0.8, random_state=0)\n",
    "    val_df   = dataset.drop(train_df.index)\n",
    "\n",
    "    X_train = train_df.drop('y_tilde', axis=1)\n",
    "    y_train = train_df['y_tilde'].to_numpy().reshape(-1, 1)\n",
    "    X_val   = val_df.drop('y_tilde', axis=1)\n",
    "    y_val   = val_df['y_tilde'].to_numpy().reshape(-1, 1)\n",
    "\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_val_scaled   = scaler.transform(X_val)\n",
    "\n",
    "    X_train_t = torch.tensor(X_train_scaled, dtype=torch.float32, device=device)\n",
    "    y_train_t = torch.tensor(y_train,       dtype=torch.float32, device=device)\n",
    "    X_val_t   = torch.tensor(X_val_scaled,  dtype=torch.float32, device=device)\n",
    "    y_val_t   = torch.tensor(y_val,         dtype=torch.float32, device=device)\n",
    "\n",
    "    train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "    train_ld = DataLoader(train_ds, batch_size=512, shuffle=True)\n",
    "\n",
    "    model = MyModel().to(device)\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    best_val = float('inf')\n",
    "    patience = 10\n",
    "    counter = 0\n",
    "    max_epochs = 500\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_ld:\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_preds = model(X_val_t)\n",
    "            val_loss = criterion(val_preds, y_val_t).item()\n",
    "\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f\"Early stop at epoch {epoch} (sim {test_id})\")\n",
    "                break\n",
    "\n",
    "    # -------------------------\n",
    "    # Save validation predictions\n",
    "    # -------------------------\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_out = model(X_val_t).cpu().numpy().flatten()\n",
    "    pd.DataFrame({'pred': val_out, 'true': y_val.flatten()}).to_csv(\n",
    "        RESULTS_DIR / f'validation_{test_id}.csv', index=False\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # Yield response function (NO EONR)\n",
    "    # -------------------------\n",
    "    test_df  = data_2nd_stage[data_2nd_stage['sim'] == test_id].reset_index(drop=True)\n",
    "    base_feats = test_df[['Nk', 'plateau', 'b0']].reset_index(drop=True)\n",
    "\n",
    "    # N sequence for THIS test_id\n",
    "    eval_seq = evall_N_seq[evall_N_seq['sim'] == test_id].reset_index(drop=True)\n",
    "    if eval_seq.empty:\n",
    "        (RESULTS_DIR / f'yield_response_{test_id}_EMPTY_EVAL_SEQ.csv').write_text(\n",
    "            \"No eval N sequence found for this sim\\n\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    Nseq = eval_seq['N'].to_numpy()\n",
    "    L = len(Nseq)\n",
    "\n",
    "    # identifiers \n",
    "    id_cols = [c for c in ['aunit_id', 'cell_id', 'field_id'] if c in test_df.columns]\n",
    "\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(base_feats)):\n",
    "            base = base_feats.iloc[[i]]                              # 1×3 (Nk, plateau, b0)\n",
    "            repeated = pd.concat([base] * L, ignore_index=True)      # L×3\n",
    "            full_feat = pd.concat([repeated, eval_seq[['N']]], axis=1)  # L×4\n",
    "            full_feat = full_feat[['Nk', 'plateau', 'b0', 'N']]      \n",
    "\n",
    "            X_feat = torch.tensor(scaler.transform(full_feat), dtype=torch.float32, device=device)\n",
    "            y_hat  = model(X_feat).cpu().numpy().reshape(-1)\n",
    "\n",
    "            row_dict = {\n",
    "                'sim':        [test_id] * L,\n",
    "                'row_id':     [i] * L,\n",
    "                'N':          Nseq,\n",
    "                'pred_yield': y_hat\n",
    "            }\n",
    "            for col in id_cols:\n",
    "                row_dict[col] = [test_df.loc[i, col]] * L\n",
    "\n",
    "            all_preds.append(pd.DataFrame(row_dict))\n",
    "\n",
    "    df_out = pd.concat(all_preds, ignore_index=True)\n",
    "    df_out.to_csv(RESULTS_DIR / f'yield_response_{test_id}.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a539c7",
   "metadata": {},
   "source": [
    "# DO_ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d6865c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test_id: 100%|██████████| 1/1 [02:23<00:00, 143.31s/it]\n"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyreadr\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# =========================\n",
    "# Project root \n",
    "# =========================\n",
    "def detect_project_root():\n",
    "    env = os.environ.get(\"CF_CONT_ROOT\")\n",
    "    if env:\n",
    "        return Path(env).expanduser().resolve()\n",
    "    try:\n",
    "        here = Path(__file__).resolve()\n",
    "    except NameError:\n",
    "        here = Path.cwd().resolve()\n",
    "    for p in [here] + list(here.parents):\n",
    "        if p.name == \"CF_Continuous\":\n",
    "            return p\n",
    "        if (p / \"data\" / \"data_20230504\").exists() and (p / \"codes\").exists():\n",
    "            return p\n",
    "    return here\n",
    "\n",
    "PROJ_ROOT = detect_project_root()\n",
    "\n",
    "# =========================\n",
    "# Paths \n",
    "# =========================\n",
    "SPLIT_DIR = PROJ_ROOT / \"data\" / \"train_test_split\"\n",
    "DATA_2ND_STAGE_RDS = PROJ_ROOT / \"data\" / \"data_20230504\" / \"data_2nd_stage.rds\"\n",
    "EVALL_N_SEQ_RDS    = PROJ_ROOT / \"data\" / \"data_20230504\" / \"evall_N_seq.rds\"\n",
    "\n",
    "# =========================\n",
    "# Output \n",
    "# =========================\n",
    "model_tag   = \"DO_ANN\"\n",
    "n_fields    = 5                 # choose 1, 5, or 10\n",
    "run_test_id = 1                 # run only this test_id\n",
    "fields_label_map = {1: \"one_field\", 5: \"five_fields\", 10: \"ten_fields\"}\n",
    "fields_label = fields_label_map.get(n_fields, f\"{n_fields}_fields\")\n",
    "\n",
    "RESULTS_BASE = PROJ_ROOT / \"results\" / \"yield_response_function_for_one_iteration\"\n",
    "RESULTS_DIR  = RESULTS_BASE / f\"YRF_{model_tag}_{fields_label}\"\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === Model  ===\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.branch = nn.Sequential(\n",
    "            nn.Linear(3, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    def forward(self, inp):\n",
    "        x = inp[:, :3]   # Nk, plateau, b0\n",
    "        n = inp[:, 3:4]  # N_tilde (scaled)\n",
    "        return self.branch(x) * n\n",
    "\n",
    "# === Load data from .rds ===\n",
    "data_2nd_stage = next(iter(pyreadr.read_r(str(DATA_2ND_STAGE_RDS)).values()))\n",
    "evall_N_seq    = next(iter(pyreadr.read_r(str(EVALL_N_SEQ_RDS)).values()))\n",
    "\n",
    "# \n",
    "for c in ['y_tilde', 'Nk', 'plateau', 'b0', 'N_tilde', 'N']:\n",
    "    if c in data_2nd_stage.columns:\n",
    "        data_2nd_stage[c] = pd.to_numeric(data_2nd_stage[c], errors='coerce')\n",
    "    if c in evall_N_seq.columns:\n",
    "        evall_N_seq[c] = pd.to_numeric(evall_N_seq[c], errors='coerce')\n",
    "\n",
    "# Clean\n",
    "data_2nd_stage = data_2nd_stage.dropna(subset=['y_tilde','Nk','plateau','b0','N_tilde']).reset_index(drop=True)\n",
    "# \n",
    "need_eval_cols = ['sim', 'N_tilde']\n",
    "if not set(need_eval_cols).issubset(evall_N_seq.columns):\n",
    "    raise ValueError(\"evall_N_seq must contain columns: 'sim' and 'N_tilde'\")\n",
    "evall_N_seq = evall_N_seq.dropna(subset=['N_tilde']).reset_index(drop=True)\n",
    "\n",
    "# === Load split CSV and restrict to one test_id ===\n",
    "split_csv_path = SPLIT_DIR / f\"train_test_splits_{n_fields}fields.csv\"\n",
    "splits_df = pd.read_csv(split_csv_path)\n",
    "splits_df = splits_df[splits_df['test_id'] == run_test_id].iloc[:1].copy()\n",
    "\n",
    "# === Features for training ===\n",
    "feature_cols = ['Nk', 'plateau', 'b0', 'N_tilde']\n",
    "\n",
    "# === Loop (only one iteration) ===\n",
    "for _, row in tqdm(splits_df.iterrows(), total=len(splits_df), desc=\"Processing test_id\"):\n",
    "    test_id = int(row['test_id'])\n",
    "    train_ids = row[[c for c in row.index if c.startswith('train_')]].values\n",
    "\n",
    "    # -------------------------\n",
    "    # train/val data\n",
    "    # -------------------------\n",
    "    dataset = data_2nd_stage[data_2nd_stage['sim'].isin(train_ids)].reset_index(drop=True)\n",
    "    dataset = dataset[['y_tilde'] + feature_cols].copy()\n",
    "\n",
    "    # Random 80/20 split (reproducible)\n",
    "    train_df = dataset.sample(frac=0.8, random_state=0)\n",
    "    val_df   = dataset.drop(train_df.index)\n",
    "\n",
    "    X_train = train_df[feature_cols]\n",
    "    y_train = train_df['y_tilde'].to_numpy().reshape(-1, 1)\n",
    "    X_val   = val_df[feature_cols]\n",
    "    y_val   = val_df['y_tilde'].to_numpy().reshape(-1, 1)\n",
    "\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_val_scaled   = scaler.transform(X_val)\n",
    "\n",
    "    X_train_t = torch.tensor(X_train_scaled, dtype=torch.float32, device=device)\n",
    "    y_train_t = torch.tensor(y_train,       dtype=torch.float32, device=device)\n",
    "    X_val_t   = torch.tensor(X_val_scaled,  dtype=torch.float32, device=device)\n",
    "    y_val_t   = torch.tensor(y_val,         dtype=torch.float32, device=device)\n",
    "\n",
    "    model = MyModel().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.L1Loss()\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 10\n",
    "    counter = 0\n",
    "    max_epochs = 500\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        # manual mini-batch\n",
    "        bs = 512\n",
    "        for i in range(0, len(X_train_t), bs):\n",
    "            xb = X_train_t[i:i+bs]\n",
    "            yb = y_train_t[i:i+bs]\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(xb), yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = criterion(model(X_val_t), y_val_t).item()\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f\"Early stop at epoch {epoch} (sim {test_id})\")\n",
    "                break\n",
    "\n",
    "    # -------------------------\n",
    "    # Save validation predictions\n",
    "    # -------------------------\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_preds = model(X_val_t).cpu().numpy().flatten()\n",
    "    pd.DataFrame({'pred': val_preds, 'true': y_val.flatten()}).to_csv(\n",
    "        RESULTS_DIR / f'validation_{test_id}.csv', index=False\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # Yield response function (NO EONR)\n",
    "    # -------------------------\n",
    "    test_df   = data_2nd_stage[data_2nd_stage['sim'] == test_id].reset_index(drop=True)\n",
    "    base_feats = test_df[['Nk', 'plateau', 'b0']].reset_index(drop=True)\n",
    "\n",
    "    # N sequence (N_tilde) for THIS test_id\n",
    "    eval_seq = evall_N_seq[evall_N_seq['sim'] == test_id].reset_index(drop=True)\n",
    "    if eval_seq.empty:\n",
    "        (RESULTS_DIR / f'yield_response_{test_id}_EMPTY_EVAL_SEQ.csv').write_text(\n",
    "            \"No eval N_tilde sequence found for this sim\\n\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    Nt_seq = eval_seq['N_tilde'].to_numpy()\n",
    "    L = len(Nt_seq)\n",
    "\n",
    "    # \n",
    "    id_cols = [c for c in ['aunit_id', 'cell_id', 'field_id'] if c in test_df.columns]\n",
    "\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(base_feats)):\n",
    "            base = base_feats.iloc[[i]]                             # 1×3\n",
    "            repeated = pd.concat([base] * L, ignore_index=True)     # L×3\n",
    "            full_feat = pd.concat([repeated, eval_seq[['N_tilde']]], axis=1)  # L×4\n",
    "            full_feat = full_feat[['Nk', 'plateau', 'b0', 'N_tilde']]         \n",
    "\n",
    "            X_feat = torch.tensor(scaler.transform(full_feat), dtype=torch.float32, device=device)\n",
    "            y_hat  = model(X_feat).cpu().numpy().reshape(-1)\n",
    "\n",
    "            row = {\n",
    "                'sim':        [test_id] * L,\n",
    "                'row_id':     [i] * L,\n",
    "                'N_tilde':    Nt_seq,         \n",
    "                'pred_yield': y_hat\n",
    "            }\n",
    "            if 'N' in eval_seq.columns:\n",
    "                row['N'] = eval_seq['N'].to_numpy()  \n",
    "            for col in id_cols:\n",
    "                row[col] = [test_df.loc[i, col]] * L\n",
    "\n",
    "            all_preds.append(pd.DataFrame(row))\n",
    "\n",
    "    df_out = pd.concat(all_preds, ignore_index=True)\n",
    "    df_out.to_csv(RESULTS_DIR / f'yield_response_{test_id}.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33e0c9b",
   "metadata": {},
   "source": [
    "# SO_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c325f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test_id: 100%|██████████| 1/1 [01:11<00:00, 71.57s/it]\n"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyreadr\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RepeatedKFold, GridSearchCV\n",
    "\n",
    "# =========================\n",
    "# Project root \n",
    "# =========================\n",
    "def detect_project_root():\n",
    "    env = os.environ.get(\"CF_CONT_ROOT\")\n",
    "    if env:\n",
    "        return Path(env).expanduser().resolve()\n",
    "    try:\n",
    "        here = Path(__file__).resolve()\n",
    "    except NameError:  # e.g., notebooks\n",
    "        here = Path.cwd().resolve()\n",
    "    for p in [here] + list(here.parents):\n",
    "        if p.name == \"CF_Continuous\":\n",
    "            return p\n",
    "        if (p / \"data\" / \"data_20230504\").exists() and (p / \"codes\").exists():\n",
    "            return p\n",
    "    return here\n",
    "\n",
    "PROJ_ROOT = detect_project_root()\n",
    "\n",
    "# =========================\n",
    "# Paths \n",
    "# =========================\n",
    "SPLIT_DIR = PROJ_ROOT / \"data\" / \"train_test_split\"\n",
    "DATA_2ND_STAGE_RDS = PROJ_ROOT / \"data\" / \"data_20230504\" / \"data_2nd_stage.rds\"\n",
    "EVALL_N_SEQ_RDS    = PROJ_ROOT / \"data\" / \"data_20230504\" / \"evall_N_seq.rds\"\n",
    "\n",
    "# =========================\n",
    "# Output \n",
    "# =========================\n",
    "model_tag   = \"SO_RF\"\n",
    "n_fields    = 5                 # choose 1, 5, or 10\n",
    "run_test_id = 1                 # run only this test_id\n",
    "fields_label_map = {1: \"one_field\", 5: \"five_fields\", 10: \"ten_fields\"}\n",
    "fields_label = fields_label_map.get(n_fields, f\"{n_fields}_fields\")\n",
    "\n",
    "RESULTS_BASE = PROJ_ROOT / \"results\" / \"yield_response_function_for_one_iteration\"\n",
    "RESULTS_DIR  = RESULTS_BASE / f\"YRF_{model_tag}_{fields_label}\"\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# === Load data from .rds ===\n",
    "data_2nd_stage = next(iter(pyreadr.read_r(str(DATA_2ND_STAGE_RDS)).values()))\n",
    "evall_N_seq    = next(iter(pyreadr.read_r(str(EVALL_N_SEQ_RDS)).values()))\n",
    "\n",
    "# \n",
    "for c in ['y_tilde','Nk','plateau','b0','N']:\n",
    "    if c in data_2nd_stage.columns:\n",
    "        data_2nd_stage[c] = pd.to_numeric(data_2nd_stage[c], errors='coerce')\n",
    "for c in ['sim','N']:\n",
    "    if c in evall_N_seq.columns:\n",
    "        evall_N_seq[c] = pd.to_numeric(evall_N_seq[c], errors='coerce')\n",
    "\n",
    "# Drop rows with NaNs \n",
    "data_2nd_stage = data_2nd_stage.dropna(subset=['y_tilde','Nk','plateau','b0','N']).reset_index(drop=True)\n",
    "evall_N_seq    = evall_N_seq.dropna(subset=['N']).reset_index(drop=True)\n",
    "\n",
    "# === Load split CSV and restrict to one test_id ===\n",
    "split_csv_path = SPLIT_DIR / f\"train_test_splits_{n_fields}fields.csv\"\n",
    "splits_df = pd.read_csv(split_csv_path)\n",
    "splits_df = splits_df[splits_df['test_id'] == run_test_id].iloc[:1].copy()\n",
    "\n",
    "# === Features for training/prediction ===\n",
    "feature_cols = ['Nk', 'plateau', 'b0', 'N']\n",
    "\n",
    "# === Loop (only one iteration) ===\n",
    "for _, row in tqdm(splits_df.iterrows(), total=len(splits_df), desc=\"Processing test_id\"):\n",
    "    test_id = int(row['test_id'])\n",
    "    train_ids = row[[c for c in row.index if c.startswith('train_')]].values\n",
    "\n",
    "    # -------------------------\n",
    "    # Train / validation data\n",
    "    # -------------------------\n",
    "    dataset = data_2nd_stage[data_2nd_stage['sim'].isin(train_ids)].reset_index(drop=True)\n",
    "    dataset = dataset[['y_tilde'] + feature_cols].copy()\n",
    "\n",
    "    train_df = dataset.sample(frac=0.8, random_state=0)\n",
    "    val_df   = dataset.drop(train_df.index)\n",
    "\n",
    "    X_train = train_df.drop('y_tilde', axis=1)\n",
    "    y_train = train_df['y_tilde']\n",
    "    X_val   = val_df.drop('y_tilde', axis=1)\n",
    "    y_val   = val_df['y_tilde']\n",
    "\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_val_scaled   = scaler.transform(X_val)\n",
    "\n",
    "    # -------------------------\n",
    "    # Random Forest with CV\n",
    "    # -------------------------\n",
    "    param_grid = {\n",
    "        'max_depth':    [3, 5],\n",
    "        'n_estimators': [50, 250, 500, 1000],\n",
    "        'max_features': [1, 2, 3],\n",
    "    }\n",
    "    rf = RandomForestRegressor(random_state=777)\n",
    "    cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=777)\n",
    "    grid = GridSearchCV(rf, param_grid, cv=cv, n_jobs=-1, scoring='neg_mean_squared_error')\n",
    "    grid.fit(X_train_scaled, y_train)\n",
    "    model = grid.best_estimator_\n",
    "\n",
    "    # -------------------------\n",
    "    # Save validation predictions\n",
    "    # -------------------------\n",
    "    val_preds = model.predict(X_val_scaled)\n",
    "    pd.DataFrame({'pred': val_preds, 'true': y_val.values}).to_csv(\n",
    "        RESULTS_DIR / f'validation_{test_id}.csv', index=False\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # Yield response function (NO EONR)\n",
    "    # -------------------------\n",
    "    test_df   = data_2nd_stage[data_2nd_stage['sim'] == test_id].reset_index(drop=True)\n",
    "    base_feats = test_df[['Nk', 'plateau', 'b0']].reset_index(drop=True)\n",
    "\n",
    "    # N sequence for THIS test_id\n",
    "    eval_seq = evall_N_seq[evall_N_seq['sim'] == test_id].reset_index(drop=True)\n",
    "    if eval_seq.empty:\n",
    "        (RESULTS_DIR / f'yield_response_{test_id}_EMPTY_EVAL_SEQ.csv').write_text(\n",
    "            \"No eval N sequence found for this sim\\n\"\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    Nseq = eval_seq['N'].to_numpy()\n",
    "    L = len(Nseq)\n",
    "\n",
    "    # \n",
    "    id_cols = [c for c in ['aunit_id', 'cell_id', 'field_id'] if c in test_df.columns]\n",
    "\n",
    "    all_preds = []\n",
    "    for i in range(len(base_feats)):\n",
    "        base = base_feats.iloc[[i]]                               # 1×3\n",
    "        repeated = pd.concat([base] * L, ignore_index=True)       # L×3\n",
    "        full_feat = pd.concat([repeated, eval_seq[['N']]], axis=1)  # L×4\n",
    "        full_feat = full_feat[['Nk', 'plateau', 'b0', 'N']]      \n",
    "\n",
    "        X_feat = scaler.transform(full_feat)\n",
    "        preds  = model.predict(X_feat)\n",
    "\n",
    "        row = {\n",
    "            'sim':        [test_id] * L,\n",
    "            'row_id':     [i] * L,\n",
    "            'N':          Nseq,\n",
    "            'pred_yield': preds\n",
    "        }\n",
    "        for col in id_cols:\n",
    "            row[col] = [test_df.loc[i, col]] * L\n",
    "\n",
    "        all_preds.append(pd.DataFrame(row))\n",
    "\n",
    "    df_out = pd.concat(all_preds, ignore_index=True)\n",
    "    df_out.to_csv(RESULTS_DIR / f'yield_response_{test_id}.csv', index=False)\n",
    "\n",
    "# --- Notes ---\n",
    "# • Trains on ['Nk','plateau','b0','N'] to predict y_tilde.\n",
    "# • Output rows = (# test rows for sim) × (length of N sequence for sim).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a74b3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

---
title: "Estimation of the effect of a continuous treatment variable"
format:
  docx:
    toc: true
    number-sections: true
    number-tables: true 
  html:
    toc: true
execute:
  echo: false
bibliography: references.bib
---


```{r}
#| include: false
library(data.table)
library(dplyr)
library(flextable)
library(officer)
library(data.table)
library(tidyverse)
library(officedown)
library(officer)
library(flextable)
library(stringr)
library(sf)
library(lfe)
library(modelsummary)
library(patchwork)
library(gridExtra)
library(kableExtra)
library(measurements)
```


# Introduction

Efficient nitrogen (N) management remains one of the most critical challenges in modern agricultural production. Nitrogen is essential for achieving high crop yields, yet excessive or poorly timed applications can lead to economic losses and environmental harm. The principle of applying the right amount of nitrogen at the right place and time requires understanding the spatial and temporal variability in crop nitrogen needs. This variability arises from complex interactions among soil nitrogen availability, soil and field characteristics, and weather conditions. Accurately identifying the optimal nitrogen rate for each site and season is therefore fundamental to improving both profitability and sustainability in crop production systems.

On-farm field experiments have emerged as a powerful approach to generate the data necessary for site-specific nitrogen management. Through on-farm agronomic trials, researchers can directly observe yield responses to varying nitrogen rates under real-world field conditions. Analyzing these data enables estimation of yield response functions and identification of site-specific economically optimal nitrogen rates. Recent advances in data analytics—particularly the widespread adoption of machine learning (ML) methods—have expanded the potential of on-farm experimentation. Two major methodological directions have emerged: (1) yield prediction–based methods, which recommend site-specific input rates by predicting yield as a function of nitrogen and environmental covariates (@krause2020random; @barbosa2020modeling; @barbosa2020risk; @gardner2021economic), and (2) causal machine learning (causal ML) methods (@kakimoto2022causal), which estimate the underlying causal relationship between nitrogen application and yield response. 

In the OFPE setting, causal machine learning methods have shown to be promising to recover the full yield response curve. @kakimoto2022causal showed that causal forest can estimate heterogeneous treatment effects of an input on yield more accurately than prediction-oriented machined approaches like random forest, boosted forest, and convolutional neural network. However, their application of CF is only limited to a single-year OFPE experiment from a single field. Causal forest was developped primarily to estimate
the heterogeneous treatment effects of a dicrete treatment. In OFPE, there are typically five
or six distinctive target experimental rates (see the first panel of @fig-distribution-N). In @kakimoto2022causal, CF estimates the effect of multiple treatments defined as the increase in nitrogen rate from the lowest rate to the other rates (so n − 1 tretments in n-rate trials). However,
once data from multiple fields with different target rates are combined, applied nitrogen rates
become rather continuous as seen in the second panel of @fig-distribution-N. Existing approaches—such as direct applications of random forests or neural networks and CF—often tend to fail to capture heterogeneous treatment effects of a continuous variable (@kakimoto2022causal). 

```{r n-histogram, include = FALSE}
# sim_data <-
#   readRDS(here::here("data/raw_sim_data.rds")) %>%
#    data.table::data.table()
# 
# reg_data_all <-
#   sim_data[, .(design_name, reg_data)] %>%
#   unnest(reg_data) %>%
#   data.table::data.table() %>%
#   .[, design := case_when(
#     design_name == "Latin Square Fixed 5" ~ "ls5",
#     design_name == "Latin Square Fixed 6" ~ "ls6"
#   )]
# 
# data_guide <-
#   lapply(
#     1:nrow(reg_data_all),
#     function(x) {
#       temp_data <- reg_data_all[x, ]
#       temp_sim <- temp_data[, sim]
#       temp_design <- temp_data[, design]
# 
#       data_file_name <- paste0("data/sim_data_", temp_sim, "_design_", temp_design, ".rds")
# 
#       saveRDS(
#         temp_data,
#         here::here(data_file_name)
#       )
# 
#       return(
#         data.table::data.table(
#           sim = temp_sim,
#           design = temp_design,
#           file_name = data_file_name
#         )
#       )
#     }
#   ) %>%
#   data.table::rbindlist()
# 
# saveRDS(data_guide, here::here("data/data_guide.rds"))

# sim_des_ls <- data_guide[, sim_des]
# train_site_num <- 20
# 
# set.seed(87924354)
# 
# test_train <-
#   data_guide %>%
#   rowwise() %>%
#   mutate(train_sim = list(
#     sample(
#       sim_des_ls[sim_des_ls != sim_des],
#       train_site_num
#     )
#   )) %>%
#   data.table::data.table()
# 
# saveRDS(test_train, here::here("data/test_train_ids.rds"))


#####

# data_guide <-
#   readRDS(here::here("data/data_guide.rds")) %>%
#   .[, sim_des := paste0(sim, "_", design)]
# 
# test_train <- readRDS(here::here("data/test_train_ids.rds"))
# 
# temp_data <- test_train[8, ]
# 
# test_sim_des <- temp_data$sim_des
# 
# #--- get the sim_des ids for training data ---#
# train_sim_des <- temp_data$train_sim[[1]]
# 
# #--- train data ---#
# train_data <-
#   data_guide[sim_des %in% train_sim_des, file_name] %>%
#   lapply(., function(x) readRDS(here::here(x))) %>%
#   data.table::rbindlist() %>%
#   .[, data] %>%
#   data.table::rbindlist() %>%
#   .[, type := "Multiple Fields"]
# 
# #--- test data ---#
# test_data <-
#   readRDS(here::here(temp_data$file_name))$data[[1]] %>%
#   .[, type := "Single Field"]
# 
# data_illust <-
#   rbind(test_data, train_data) %>%
#   .[, type := factor(type, levels = c("Single Field", "Multiple Fields"))]
# 
# (
#   g_N_hist_illust <-
#     ggplot(data_illust) +
#     geom_histogram(
#       aes(x = N),
#       bins = 50,
#       fill = "gray",
#       color = "blue"
#     ) +
#     facet_grid(type ~ ., scales = "free_y") +
#     xlab("Nitrogen Application Rate (lb/acre)") +
#     ylab("Count")
# )


# ggsave(
#   g_N_hist_illust,
#   file = here::here("writing/Figures/g_N_hist_single_multiple.pdf"),
#   width = 6.5,
#   height = 4
# )


```

```{r fig-distribution-N, fig.cap="Distribution of nitrogen rates from a single OFPE and multiple OFPEs"}
knitr::include_graphics(here::here("writing/Figures/g_N_hist_single_multiple.pdf"))
```
As multi-year, multi-field OFPE datasets accumulate, pooling data across fields has become both feasible and desirable—it can improve estimation of yield response functions and deliver stronger agronomic and economic insights. It creates a need for methods that handle continuous treatments and it becomes especially important to adapt causal methods to continuous treatments without losing their statistical strengths over other machine-learning approaches.

The key objective of this article is to identify a causal machine learning approach capable of estimating heterogeneous treatment effects for yield response function estimation using data from on-farm trials, where the treatment variable—nitrogen rate—is continuous and randomized. We investigate two complementary approaches: a Single-Orthogonalized Neural Network (SO-ANN), designed to flexibly capture nonlinear treatment–response relationships, and an extended Causal Forest (CF) that incorporates smoothing splines to model continuous treatments. Both methods aim to preserve the strengths of causal ML—nonparametric modeling of complex interactions without restrictive assumptions. Their performance is evaluated through Monte Carlo simulations that mimic typical OFPE conditions and crop yield responses to nitrogen.

We find that the SO-ANN approach outperforms the non-orthogonalized model, which represents the common practice in yield response function estimation. In contrast, the Double-Orthogonalized approach performs poorly, and its accuracy does not improve even with larger sample sizes, consistent with theoretical expectations. Our extended CF method successfully captures heterogeneous and nonlinear effects of a continuous treatment and accurately estimates site-specific profit-maximizing nitrogen rates with minimal bias underscoring the promise of this approach. Although the methodological extension proposed here is primarily motivated by the needs of OFPE, it holds potential for a wide range of applications in applied economics where continuous treatments and heterogeneous effects are of interest.


# Method

## Models and estimation procedures

The standard approach in the precision-agriculture literature for estimating site-specific input response is to model the outcome level directly as a function of the treatment/input and covariates—e.g., estimate $Y=f(I,C)$ where $I$ is the input (like nitrogen rate) and $C$ are field characteristics—and then derive optimal input decisions from that estimated function. This is what is typically done in the recent on-farm precision experimentation literature (e.g., @barbosa2020risk; @barbosa2020modeling; @krause2020random; @gardner2021economic).
The problem is that these methods optimize prediction of yield levels—not the causal effect of changing the input. Good level prediction at the observed input does not guarantee accurate recovery of the response function needed for optimal input choice, because the fitted $f(I,C)$ can conflate input variation with confounding and misattribute the drivers of yield variation.

To identify causal effects rather than just predict levels in the single binary treatment case, researchers use double/debiased machine learning (DML), which introduces orthogonalization so the treatment-effect estimator is insensitive to small errors in the auxiliary fits (@chernozhukov2018double; @mackey2018orthogonal). 
Consider the structural model (Model 1):

```{=tex}
\begin{align*}
Y=\theta(X) T+g(X, W)+v
\end{align*}
```

where $Y$ is the outcome, $T \in\{0,1\}$  is the treatment, $X$ covariates that also modify the treatment effect $\theta(X)$, $W$ additional predictors of $Y$, and $v$ is noise. 
Orthogonalization estimates $f(X, W)=\mathbb{E}[Y \mid X, W]$ and $h(X, W)=\mathbb{E}[T \mid X, W]$, forms residuals $Y_{\mathrm{res}}=Y-f(X,W)$ and $T_{\mathrm{res}}=T-h(X, W)$, and then relates $Y_{\mathrm{res}}$ to $T_{\mathrm{res}}$. Because this score is Neyman-orthogonal and is typically implemented with cross-fitting, small errors in $f$ and $h$ do not induce first-order bias in $\theta(X)$.

The identifying algebra in Model 1 follows from linearity in $T$:

```{=tex}
\begin{align*}
\mathbb{E}[Y \mid X, W]=\theta(X) \mathbb{E}[T \mid X, W]+g(X, W)=\theta(X) h(X, W)+g(X, W)
\end{align*}
```

so

```{=tex}
\begin{align*}
Y_{\mathrm{res}}=Y-\mathbb{E}[Y \mid X, W]=\theta(X)(T-h(X, W))+v=\theta(X) T_{\mathrm{res}}+v
\end{align*}
```

Thus regressing $Y_{\mathrm{res}}$ on $T_{\mathrm{res}}$ recovers $\theta(X)$ up to noise.

@kakimoto2022causal applied this framework to estimate heterogeneous treatment effects of nitrogen on yield and treated heterogeneity in a discrete way. However, when data are pooled across fields and applied nitrogen rates become continuous, the standard residualization of the treatment in DML does not extend cleanly to that setting.

Consider the continuous-treatment setting, where the structural model is

```{=tex}
\begin{align*}
Y=\theta(X, T)+g(X, W)+v
\end{align*}
```

with $T$ (e.g., nitrogen rate) continuous, and $X$ heterogeneity-inducing covariates that interact with $T$ through the flexible response function $\theta(X, T)$. In this case double-orthogonalization fails because $\theta$ is a general (nonlinear and interaction-rich) function of $T$. Residualizing the treatment by replacing $T$ with $T-\mathbb{E}[T \mid X, W]$ does not produce a valid adjustment for the heterogeneous effect. Formally,

```{=tex}
\begin{align*}
\theta(X, T)-\mathbb{E}[\theta(X, T) \mid X, W] \neq \theta(X, T-\mathbb{E}[T \mid X, W])
\end{align*}
```

and the clean decomposition that underlies double-orthogonalization in the linear-in-$T$ case breaks down. 
Instead, we apply single-orthogonalization (SO). We first estimate $f(X, W)=\mathbb{E}[Y \mid X, W]$ and form the residual $Y_{\mathrm{res}}=Y-f(X, W)$, and then recover $\theta(X, T)$ by flexibly modeling  $Y_{\mathrm{res}}$ as a function of $(X,T)$. This approach removes baseline variation due to $g(X,W)$ while preserving the full interaction structure between $X$ and the continuous treatment.

To study and compare these ideas systematically, we adopt a single underlying structural model and then implement alternative orthogonalization schemes.

The model of interest throughout is (Model 3)

```{=tex}
\begin{align*}
y = f(X, N) + g(W) + v
\end{align*}
```

where $y$ is crop yield, $N$ is the continuous nitrogen rate, $X$ is a set of covariates that induce heterogeneity in the impact of $N$ on $y$, and $W$ is a set of covariates that affect $y$ but do not interact with $N$. 
We implement three estimation strategies—double-orthogonalized, single-orthogonalized, and non-orthogonalized—that differ only in how (and whether) the outcome and treatment are residualized before the final fit. Orthogonalization means fitting a variable on its conditioning covariates and using the residual (observed minus predicted) so that the subsequent learner focuses on variation not explained by those covariates; all residualizations are performed with random forests. In the double-orthogonalized approach, both $y$ and $N$ are residualized with respect to $(X,W)$ and the final learner is trained on those residuals. In the single-orthogonalized approach, only $y$ is residualized and the raw $N$ is retained. The non-orthogonalized specification skips residualization entirely and fits the learner directly to $(y,N,X,W)$. For each strategy we consider three final-stage learners—neural networks (NN), random forests (RF), and causal forests (CF)—to evaluate how the orthogonalization scheme and learning algorithm interact in recovering heterogeneous nitrogen response.


The table below shows all the cases we consider.


```{r}
#| include: false
f_est <-
  data.table(
    "Approach Label" = c("DO-NN", "SO-RFF", "SO-NN", "DO-CF","NO-NN", "NO-RF"),
    "Approach Type" = c("Double-orthogonalized", "Single-orthogonalized", "Single-orthogonalized", "Double-orthogonalized", "Non-orthogonalized", "Non-orthogonalized"),
    "Y-orthogonalization" = c("Randome Forest", "Randome Forest", "Randome Forest", "Randome Forest", "NA", "NA"),
    "T-orthogonalization" = c("Randome Forest", "NA", "NA", "Randome Forest", "NA", "NA"),
    "Final stage" = c("Neural Network", "Random Forest", "Neural Network", "Causal Forest", "Neural Network", "Random Forest")
  ) %>%
  flextable() %>%
  fontsize(size = 16, part = "all") %>%
  autofit()

flextable::save_as_image(f_est, here::here("writing/f_est.png"), res = 300)
```

```{r}
#| echo: false
#| fig-width: 4
#| out-width: 4in
knitr::include_graphics(here::here("writing/f_est.png"))
```

### Single-orthogonalized NN (SO-NN) and (SO-RF)

In the single-orthogonalized framework only the baseline component $g(W)$ is partialed out before the final-stage estimation; the continuous treatment $N$ and heterogeneity-inducing covariates $X$ remain in their original form. Formally, starting from Model 3, we first estimate $g(W)=\mathbb{E}[y \mid W]$ using a cross-fitted random forest and form the residualized outcome

```{=tex}
\begin{align*}
\tilde{y}=y-\widehat{g}(W)
\end{align*}
```

The second stage fits a flexible model of $\tilde{y}$ on $(X,N)$ to recover the heterogeneous response function $f(X,N)$. The distinction between SO-NN and SO-RF is only in the choice of the final learner: SO-NN uses a neural network, while SO-RF uses a random forest. In both cases the learner estimates $\hat{f}(X, N)$, which is then used to predict yield under candidate nitrogen rates. Marginal profit is computed over a grid of $N$ values and the economically optimal nitrogen rate is selected as the one that maximizes 

```{=tex}
\begin{align*}
\widehat{\mathrm{EONR}}=\arg \max _N\left[P_{\text {Corn }} \times \hat{f}(X, N)-P_N \times N\right]
\end{align*}
```
where $P_{\text {Corn }}$ is the corn price, $P_{\text {N }}$ is the nitrogen price, and $\hat{f}$ is the estimated yield response.

### Double-orthogonalized CF (DO-CF) and ANN (DO-ANN) 

#### DO‑CF (continuous)

To adapt double-orthogonalization to the model of interest (Model 3), we estimate the heterogeneous response $f(X,N)$ by expanding the continuous treatment $N$ in a set of spline basis functions and allowing their coefficients to vary with the heterogeneity-inducing covariates $X$:

```{=tex}
\begin{align*}
y=\sum_{k=1}^K \theta_k(X) S_k(N)+g(X, W)+v
\end{align*}
```

Although double-orthogonalization generally fails when $f(X,N)$ is an arbitrary nonlinear function of $N$, this spline-based formulation restores the key linear-in-parameter structure that makes DO valid, because the model is linear in the transformed treatment basis $\left\{S_k(N)\right\}$. Residualizing each basis component and the outcome with respect to $(X,W)$ then yields a clean decomposition analogous to the linear-in-$T$ case. Let $\widetilde{S}_k(N)=S_k(N)-\mathbb{E}\left[S_k(N) \mid X, W\right]$. Then

```{=tex}
\begin{align*}
\tilde{y}=y-\mathbb{E}[y \mid X, W]=\sum_{k=1}^K \theta_k(X) \widetilde{S}_k(N)+v,
\end{align*}
```

so causal forest can nonparametrically estimate each heterogeneous weight $\theta_k(X)$  by relating the residualized outcome to the residualized spline bases. This Neyman-orthogonal (R-learner) style decomposition preserves the ability to capture complex, heterogeneous, and nonlinear treatment effects while inheriting the robustness of double-orthogonalization: small errors in the nuisance fits for $g(X,W)$ or the conditional means of the bases do not induce first-order bias in $\theta_k(X)$. The estimated $\theta_k(X)$ then combine to recover the full heterogeneous response function and, by differencing across candidate $N$ values, site-specific treatment effects and economically optimal nitrogen rates.

#### DO‑ANN

In the DO-ANN approach, we apply double-orthogonalization to Model 3 by residualizing both the outcome $y$ and the continuous treatment $N$ with respect to $(X, W)$ using cross-fitted random forests:

```{=tex}
\tilde{y}=y-\widehat{\mathbb{E}}[y \mid X, W], \quad \tilde{N}=N-\widehat{\mathbb{E}}[N \mid X, W]
\end{align*}
```

The final-stage learner is a neural network that models $\tilde{y}$ as a linear function of $\tilde{N}$, with heterogeneity in the slope captured by covariates $X$:

```{=tex}
\hat{\tilde{y}}=\hat{\theta}(X) \cdot \tilde{N}
\end{align*}
```

This structure preserves the linear-in-treatment-residual form required for valid double-orthogonalization while allowing $\hat{\theta}(X)$ to vary flexibly across sites. The neural network receives $X$ as input and outputs the site-specific treatment effect.

After training, we reconstruct $\hat{f}(X, N)$ over a grid of candidate nitrogen rates by evaluating the model across values of $N$, and select the economically optimal nitrogen rate.

### Non-orthogonalized ANN (NO-NN) and RF (NO-RF)

In the non‑orthogonalized setting, neither the outcome variable nor the treatment is residualized prior to estimation. Instead, the model directly fits the observed yield on both the nitrogen rate and covariates. We estimate the response function $\hat{Y}=\hat{f}(X, N)$ using the original values of $Y$, $N$ and $X$, without any orthogonalization. As in the single‑orthogonalized approaches, the function $\hat{f}(X, N)$ is estimated non‑parametrically. The difference between NO‑NN and NO‑RF lies in the choice of final-stage learner: NO‑NN uses a neural network, while NO‑RF relies on a random forest. Once the yield response function is estimated, the EONR is determined using the same profit-maximization procedure described in earlier models.


## Performance Measurement

For each simulation round, we evaluate model performance by calculating the root mean squared error (RMSE) of EONR predictions and the associated profit loss for each method. The RMSE of EONR predictions in a given simulation round is computed as follows:


```{=tex}
\begin{align*}
RMSE_{EONR}=\sqrt{\frac{1}{n} \sum_{i=1}^n\left(\widehat{EONR}_i-EONR_i\right)^2}
\end{align*}
```

To assess the predictive accuracy and economic relevance of these modeling approaches, we design a set of simulation experiments that reflect realistic agronomic conditions. These simulations serve as a controlled environment for evaluating model performance in estimating yield response functions and recommending optimal nitrogen rates.

## Simulation Scenarios

To evaluate the performance of the proposed models we design Monte Carlo simulations that mimic a typical on-farm precision experimentation (OFPE) setting. These simulations examine the ability of each method to accurately estimate the site-specific yield response to nitrogen application and the EONR under varying data availability conditions.

Our simulation consists of 500 independently generated fields, each representing a distinct agronomic environment with field-specific parameters that shape the crop’s response to nitrogen application. These parameters vary systematically across fields to introduce heterogeneity in yield responses. The true yield response in each field follows a quadratic-plateau functional form, a widely used structure for modeling nitrogen response curves in agronomic studies.

For each simulation round, one field is selected as the test field. The remaining fields are used to construct the training set in three scenarios: (1) one-field training, (2) three-field training, (3) five-field training, (4) ten-field training and (5) twenty-field training. These scenarios allow us to assess how model performance changes with the size and diversity of training data. Within each round, the model is trained on the designated number of training fields and then used to estimate the yield response function and the site-specific EONR for the test field.

The experimental design within each field consists of randomized nitrogen treatment rates applied across plots. The nitrogen rates vary between fields and are drawn to reflect realistic trial setups that span a range of agronomic optima. For evaluation purposes, we use a grid of candidate nitrogen rates to compute predicted yield and associated marginal profit for each observation. The predicted EONR is the nitrogen rate that maximizes predicted profit.

## Data generation

First, a 92.17-acre rectangular field is created, which consists of $384$ of `r measurements::conv_unit(18.3, "m", "ft") %>% round(digits = 0)` feet $\times$ `r measurements::conv_unit(73.2, "m", "ft") %>% round(digits = 0)` feet plots. each of which will be assigned an N fertilizer application rate. Each plot is made up of four (4-rows $\times$ 1-column) `r measurements::conv_unit(18.3, "m", "ft") %>% round(digits = 0)` feet $\times$ `r measurements::conv_unit(18.3, "m", "ft") %>% round(digits = 0)` feet "subplots," which are the unit of analysis used in subsequent statistical analysis. Each subplot consists of a 6-rows $\times$ 6-columns grid of thirty-six `r measurements::conv_unit(3.05, "m", "ft") %>% round(digits = 0)` feet $\times$ `r measurements::conv_unit(3.05, "m", "ft") %>% round(digits = 0)` feet "cells." 
Visual representation of the field layout is presented in @fig-field-layout. The data are generated at the cell-level first and then aggregated up to analysis-unit level.

```{r}

# /*+++++++++++++++++++++++++++++++++++
#' ## Plots
# /*+++++++++++++++++++++++++++++++++++

#library(patchwork)


# all_data <- readRDS(here::here("data/raw_sim_data.rds"))
# field_sf <- all_data$field_sf[[1]]
# 
# 
# plot_sf <-
#   field_sf %>%
#   nest_by(plot_id) %>%
#   mutate(data = list(
#     st_union(data)
#   )) %>%
#   unnest() %>%
#   st_as_sf()
# 
# plot_focus <- filter(plot_sf, plot_id == 283)
# 
# plot_focus_xy <-
#   st_centroid(plot_focus) %>%
#   st_coordinates()
# 
# bbox_field <- st_bbox(plot_sf)
# min_x_field <- bbox_field["xmin"]
# max_x_field <- bbox_field["xmax"]
# center_x_field <- mean(c(min_x_field, max_x_field))
# min_y_field <- bbox_field["ymin"]
# max_y_field <- bbox_field["ymax"]
# center_y_field <- mean(c(min_y_field, max_y_field))
# 
# g_field <-
#   ggplot() +
#   geom_sf(data = plot_sf, linewidth = 1, fill = NA) +
#   geom_sf(data = plot_focus, fill = "red", alpha = 0.5) +
#   #--- horizontal width ---#
#   geom_segment(
#     aes(x = center_x_field - 72, xend = min_x_field, y = max_y_field + 18, yend = max_y_field + 18),
#     arrow = arrow(length = unit(0.3, "cm"))
#   ) +
#   geom_segment(
#     aes(x = center_x_field + 72, xend = max_x_field, y = max_y_field + 18, yend = max_y_field + 18),
#     arrow = arrow(length = unit(0.3, "cm"))
#   ) +
#   annotate(
#     "text",
#     x = center_x_field,
#     y = max_y_field + 18,
#     label = "2,835 feet"
#   ) +
#   #--- vertical length ---#
#   geom_segment(
#     aes(x = max_x_field + 18, xend = max_x_field + 18, y = center_y_field + 60, yend = max_y_field),
#     arrow = arrow(length = unit(0.3, "cm"))
#   ) +
#   geom_segment(
#     aes(x = max_x_field + 18, xend = max_x_field + 18, y = center_y_field - 60, yend = min_y_field),
#     arrow = arrow(length = unit(0.3, "cm"))
#   ) +
#   annotate(
#     "text",
#     x = max_x_field + 18,
#     y = center_y_field,
#     label = "1,417 feet",
#     angle = "270"
#   ) +
#   #--- expansion to the focus plot ---#
#   geom_segment(
#     aes(x = plot_focus_xy[1, "X"] - 36, xend = min_x_field, y = plot_focus_xy[1, "Y"] - 9, yend = plot_focus_xy[1, "Y"] - 120),
#     color = "red"
#   ) +
#   geom_segment(
#     aes(x = plot_focus_xy[1, "X"] + 36, xend = max_x_field, y = plot_focus_xy[1, "Y"] - 9, yend = plot_focus_xy[1, "Y"] - 120),
#     color = "red"
#   ) +
#   annotate(
#     "text",
#     y = plot_focus_xy[1, "Y"],
#     x = plot_focus_xy[1, "X"],
#     label = "Plot",
#     size = 3
#   ) +
#   theme_void()
# 
# # /*+++++++++++++++++++++++++++++++++++
# #' ## inside a plot
# # /*+++++++++++++++++++++++++++++++++++
# plot_sf_focus <- filter(field_sf, plot_id == 1)
# 
# bbox_plot <- st_bbox(plot_sf_focus)
# min_x_plot <- bbox_plot["xmin"]
# max_x_plot <- bbox_plot["xmax"]
# center_x_plot <- mean(c(min_x_plot, max_x_plot))
# min_y_plot <- bbox_plot["ymin"]
# max_y_plot <- bbox_plot["ymax"]
# center_y_plot <- mean(c(min_y_plot, max_y_plot))
# 
# subplot_sf <-
#   plot_sf_focus %>%
#   nest_by(aunit_id, buffer) %>%
#   mutate(data = list(
#     st_union(data)
#   )) %>%
#   unnest() %>%
#   st_as_sf() %>%
#   mutate(label = ifelse(
#     buffer == 1,
#     "buffer",
#     paste0("subplot-", aunit_id - 1)
#   )) %>%
#   mutate(buf_or_not = ifelse(label == "buffer", "buffer", "subplot"))
# 
# subplot_text_sf <- st_centroid(subplot_sf)
# 
# cell_sf <- plot_sf_focus[7, ]
# 
# bbox_cell <- st_bbox(cell_sf)
# min_x_cell <- bbox_cell["xmin"]
# max_x_cell <- bbox_cell["xmax"]
# center_x_cell <- mean(c(min_x_cell, max_x_cell))
# min_y_cell <- bbox_cell["ymin"]
# max_y_cell <- bbox_cell["ymax"]
# center_y_cell <- mean(c(min_y_cell, max_y_cell))
# 
# g_inside_plot <-
#   ggplot() +
#   geom_sf(data = plot_sf_focus, size = 0.2, fill = NA) +
#   #--- horizontal width ---#
#   geom_segment(
#     aes(x = center_x_plot - 6, xend = min_x_plot, y = min_y_plot - 3, yend = min_y_plot - 3),
#     arrow = arrow(length = unit(0.3, "cm"))
#   ) +
#   geom_segment(
#     aes(x = center_x_plot + 6, xend = max_x_plot, y = min_y_plot - 3, yend = min_y_plot - 3),
#     arrow = arrow(length = unit(0.3, "cm"))
#   ) +
#   annotate(
#     "text",
#     x = center_x_plot,
#     y = min_y_plot - 3,
#     label = "236 feet"
#   ) +
#   geom_sf(data = filter(subplot_sf, buffer == 1), fill = "yellow", alpha = 0.4) +
#   geom_sf(data = subplot_sf, size = 1.2, fill = NA, color = "blue") +
#   geom_sf_text(data = subplot_text_sf, aes(label = label), size = 4, color = "blue") +
#   geom_sf(data = cell_sf, fill = "green", alpha = 0.3) +
#   #--- horizontal width of the cell ---#
#   geom_segment(
#     aes(x = center_x_cell, xend = min_x_cell, y = max_y_cell + 1, yend = max_y_cell + 1),
#     arrow = arrow(length = unit(0.2, "cm"))
#   ) +
#   geom_segment(
#     aes(x = center_x_cell, xend = max_x_cell, y = max_y_cell + 1, yend = max_y_cell + 1),
#     arrow = arrow(length = unit(0.2, "cm"))
#   ) +
#   annotate(
#     "text",
#     x = center_x_cell,
#     y = max_y_plot + 3,
#     label = "19.7 feet"
#   ) +
#   annotate("text", x = 39, y = 429, label = "cell") +
#   scale_fill_discrete(name = "") +
#   theme_void() +
#   theme(
#     legend.position = "bottom",
#     legend.text = element_text(size = 12)
#   )
# 
# g_layout <- g_field / g_inside_plot
# 
# saveRDS(g_layout, here::here("writing/Figures/g_layout.rds"))
# 
# ggsave(
#   g_layout,
#   file = here::here("writing/Figures/g_layout.pdf"),
#   width = 6.5,
#   height = 6
# )
```

```{r fig-field-layout, fig.cap = "Visualization of the field layout and components"}
knitr::include_graphics(here::here("Writing/Figures/g_layout.pdf"))
```
Cell-level yields are generated following the quadratic-plateau functional form as follows:

```{=latex}
\begin{align} 
y_{i,j} = 
  \begin{cases}
  \alpha_{i,j} + \beta_{i,j} N + \gamma_{i,j} N^2 + \varepsilon_{i,j}, & N < \tau_{i,j} \\
  \eta_{i,j} + \varepsilon_{i,j}, & N \geq \tau_{i,j}
  \end{cases}
\label{eq:yield-gen-sim}
\end{align}
```

In this formulation, yield increases as $N_{i,j}$ increase until $N_{i,j}$ reaches $\tau_{i,j}$, at which yield hits the plateau, $\eta_{i,j}$. The yield level at $N_{i,j}= 0$ is $\alpha_{i,j}$. The rate at which yield increases with respect to $N_{i,j}$ is governed by $\beta_{i,j}$ and $\gamma_{i,j}$. For a given cell, the quadratic-plateu function has five parameters, but the degree of freedom is only three. That is, once any three of the five parameters are determined, then the rest of the parameters are automatically determined. Specifically, we first generated $\alpha_{i,j}$, $\eta_{i,j}$, and $\tau_{i,j}$ in a spatially correlated manner following the Gaussian bluh bluh process (cite), and then $\beta_{i,j}$ and $\gamma_{i,j}$ were derived. The erorr term ($\varepsilon_{i,t}$) is generated using the same spatial process. This is repeated independenty 500 times to create 500 fields of the same size but different spatial distribution and values of the three parameters.

Trial designs vary field by field based on the value of $\tau_{i,j}$. Specifically, the lowest nitrogen rate is set at the minimum of $\tau_{i,j}$ of the field minus 50, and the highest rate is set at the maximum of $\tau_{i,j}$ of the field plus 30. We then create a sequence of equi-distance 5-level nitrogen rates. Once the unique nitrogen levels are determined, a Latin square design is used as a trial design (see  @fig-trial-design) for an example trial design). This ensures that nitrogen rate is orthogonal to any variable including the error term, avoiding endogeneity problem as done in OFPE in real world. Now, yield values at the cell level can be calculated by plugging in the value of $N_{i,j}$, $\alpha_{i,j}$, $\eta_{i,j}$, $\tau_{i,j}$, and $\varepsilon_{i,j}$ into Equation~\ref{eq:yield-gen-sim}.


```{r}
# trial_design <-
#   all_data$trial_design[[1]] %>%
#   .[sim == 1, ]
# 
# field <- data.table(field_sf)
# 
# td_sf <-
#   trial_design[field, on = c("block_id", "plot_in_block_id")] %>%
#   st_as_sf()
# 
# g_td <-
#   ggplot(td_sf) +
#   geom_sf(aes(fill = factor(N_tgt)), color = NA) +
#   scale_fill_viridis_d(name = "Nitrogen Rate (lb/acre)") +
#   theme_void() +
#   theme(legend.position = "bottom")
# 
# saveRDS(g_td, here::here("writing/Figures/g_td.rds"))
```

```{r fig-trial-design, fig.cap = "Example trial design"}
readRDS(here::here("writing/Figures/g_td.rds"))
```

# Simulation Results 

We compare six estimators—CF, SO_ANN, DO_ANN, NO_ANN, SO_RF, and NO_RF—over 500 Monte-Carlo replications and five training sizes (1, 3, 5, 10 and 20 fields). Each replication contains 1,440 evaluation units. Accuracy is assessed by the RMSE of the estimated economically optimal nitrogen rate (EONR) with respect to the true (simulated) EONR. 


```{r sim-setup, include=FALSE}

knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)


suppressPackageStartupMessages(library(here))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(stringr))

# load data
rmse_tbl_raw <- readRDS(here::here("results", "rmse_tbl_raw.rds"))


```

Taken together, @fig-rmse-hist and @fig-avg-performance illustrate clear performance patterns across methods and training sizes. CF delivers low and tightly concentrated RMSE distributions even with limited data—consistently maintaining strong accuracy and stability for 1–5 fields (mean RMSE ≈ 14.7, 13.1, and 12.1, respectively), with only modest additional gains at 10 and 20 fields (11.9–11.8). SO_ANN initially lags behind CF (18.4 at one field), but its mean RMSE decreases steadily as more fields are added, surpassing CF performance at 10 and 20 fields (11.2 and 8.5, respectively). This pattern underscores the neural network’s superior ability to exploit richer spatial and agronomic heterogeneity when sufficient training data are available.

By contrast, NO_ANN exhibits substantial instability in the single-field case, with a heavy right tail and much higher errors, but rapidly converges toward the leading methods by 5–10 fields (mean RMSE improving to around 14–13), highlighting its strong dependence on sample size. SO_RF improves modestly with more fields (mid-20s RMSE for 10–20 fields) but remains consistently less accurate than SO_ANN or CF. NO_RF maintains broad, right-shifted distributions and higher mean errors throughout (≈ 36–40), confirming its role as a weak baseline. Finally, DO_ANN performs poorly throughout, with both histograms and means showing persistently high errors (≈ 90+) and negligible benefit from additional data, reflecting over-correction in the orthogonalization process.


```{r}
#| label: fig-rmse-hist
#| fig-cap: "RMSE histograms by model and number of fields."
#| fig-width: 12
#| fig-height: 7
#| fig-show: hold

library(dplyr)
library(ggplot2)
library(stringr)

model_names <- function(models) {
  map <- c(
    "Simple_ANN" = "No_ANN",
    "RANN"       = "DO_ANN",
    "HALFRANN"   = "SO_ANN",
    "HALFRRF"    = "SO_RF",
    "NO_RF"      = "NO_RF",
    "CF"         = "CF"
  )
  models_upper <- toupper(models)
  out <- map[models_upper]
  ifelse(is.na(out), models, out)
}

rmse_plot_df <- rmse_tbl_raw %>%
  filter(ok_rows) %>%
  mutate(
    row_lab = model_names(Model),
    col_lab = factor(`#field`,
      levels = sort(unique(`#field`)),
      labels = paste0("Number of fields: ", sort(unique(`#field`)))
    )
  )

# prevent table output from creating phantom figure
invisible(rmse_plot_df %>% count(row_lab, col_lab))

xmin <- 0
nice_ceiling <- function(x, base = 5) base * ceiling(x / base)
xmax <- nice_ceiling(max(rmse_plot_df$RMSE, na.rm = TRUE), base = 5)
bins <- 25
binwidth <- (xmax - xmin) / bins

p <- ggplot(rmse_plot_df, aes(x = RMSE)) +
  geom_histogram(binwidth = binwidth, boundary = 0, closed = "left", fill = "skyblue3") +
  facet_grid(row_lab ~ col_lab, scales = "fixed") +
  scale_x_continuous(limits = c(xmin, xmax), breaks = seq(xmin, xmax, length.out = 6), expand = c(0, 0)) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) +
  labs(
    #title = "Distribution of RMSE across models and field counts",
    x = "RMSE by iteration",
    y = "Count"
  ) +
  theme_bw() +
  theme(
    strip.text = element_text(face = "bold", size = 14),
    axis.title = element_text(size = 13),
    plot.title = element_text(hjust = 0.5, face = "bold", size = 16)
  )

print(p)

```



```{r}
#| label: fig-avg-performance
#| fig-cap: "Average performance by model and number of fields."
#| fig-width: 12
#| fig-height: 7

rmse_df <- rmse_tbl_raw %>%
  dplyr::group_by(Model, `#field`, sim) %>%  
  dplyr::summarise(RMSE = dplyr::first(RMSE), .groups = "drop") %>%
  dplyr::mutate(
    # Field labels (auto from unique values)
    field_f = factor(
      `#field`,
      levels = sort(unique(`#field`)),
      labels = paste0(sort(unique(`#field`)),
                      ifelse(sort(unique(`#field`)) == 1, " field", " fields"))
    )
  )


# 3) summary stats 
rmse_summary <- rmse_df %>%
  dplyr::group_by(Model, field_f) %>%
  dplyr::summarise(
    n      = dplyr::n(),
    mean   = mean(RMSE, na.rm = TRUE),
    median = median(RMSE, na.rm = TRUE),
    sd     = sd(RMSE, na.rm = TRUE),
    se     = sd / sqrt(n),
    lower  = mean + qt(0.025, df = n - 1) * se,
    upper  = mean + qt(0.975, df = n - 1) * se,
    .groups = "drop"
  )
# 
p_mean <- ggplot(rmse_summary, aes(x = field_f, y = mean, group = Model, color = Model)) +
  geom_line(linewidth = 0.7, alpha = 0.8) +
  geom_point(size = 3) +
  labs(x = "# fields", y = "Mean RMSE", color = "Model") +
  theme_bw() +
  theme(
    axis.title = element_text(size = 13),
    strip.text = element_text(face = "bold")
  )

p_mean


# ggsave(here::here("results","rmse_mean_by_model_fields.png"), p_mean, width = 9, height = 5, dpi = 300)
```

```{r}
# #| label: tbl-mean-rmse-heatmap
# #| tbl-cap: "Mean RMSE by model and number of fields."
# #| tbl-cap-location: bottom
# #| echo: false
# #| message: false
# #| warning: false
# 
# library(dplyr)
# library(tidyr)
# library(gt)
# library(scales)
# 
# # Range for coloring
# fill_min <- floor(min(rmse_summary$mean, na.rm = TRUE))
# fill_max <- ceiling(max(rmse_summary$mean, na.rm = TRUE))
# 
# # Wide table of means
# rmse_table <- rmse_summary %>%
#   select(Model, field_f, mean) %>%
#   mutate(mean = round(mean, 1)) %>%
#   pivot_wider(names_from = field_f, values_from = mean) %>%
#   arrange(Model)
# 
# # Light → mid-blue palette
# pal <- col_numeric(
#   palette = c("#f7fbff", "#6baed6"),
#   domain  = c(fill_min, fill_max)
# )
# 
# # HTML colorbar legend to place under the table
# legend_html <- sprintf(
#   '<div style="display:flex;align-items:center;gap:8px;margin-top:6px;">
#      <span style="font-size:12px;color:#444;">Low (%.1f)</span>
#      <div style="flex:1;height:10px;background:linear-gradient(to right,#f7fbff,#6baed6);
#                  border:1px solid #ddd;border-radius:3px;"></div>
#      <span style="font-size:12px;color:#444;">High (%.1f)</span>
#    </div>', fill_min, fill_max)
# 
# rmse_table %>%
#   gt(rowname_col = "Model") %>%
#   data_color(
#     columns = c(`1 field`, `5 fields`, `10 fields`),
#     colors  = pal
#   ) %>%
#   fmt_number(columns = c(`1 field`, `5 fields`, `10 fields`), decimals = 1) %>%
#   cols_label(
#     `1 field`  = "1 field",
#     `5 fields` = "5 fields",
#     `10 fields`= "10 fields"
#   ) %>%
#   tab_options(
#     table.font.size = px(13),
#     data_row.padding = px(6),
#     column_labels.font.weight = "bold"
#   ) %>%
#   tab_source_note(source_note = gt::html(legend_html))
```

@fig-improvement reports each method’s percentage improvement in mean RMSE relative to the NO_RF baseline. CF consistently provides the strongest gains among the robust approaches when data are scarce—reducing error by ≈63% with a single field, and climbing to ≈67–68% once 5–20 fields are available. SO_ANN exhibits a similar improvement trend but grows even more dominant as more diverse data accumulate, achieving ≈53% improvement at 1 field, ≈63% by 5 fields, and reaching ≈68% at 10 fields and ≈77% at 20 fields, the highest improvement across all models. NO_ANN transitions from a large negative result when trained on just one field (substantially worse than baseline) to strong relative improvements exceeding 65% by 10–20 fields, reinforcing its reliance on adequate sample support. SO_RF offers moderate improvement (≈38–43%) that changes little with added fields. DO_ANN remains entirely uncompetitive across all sample sizes—showing worsening performance relative to NO_RF by more than 100–150%, indicating severe model misspecification.

Together, these results emphasize that while CF is the most reliable option under limited data availability, SO_ANN ultimately yields the largest relative error reduction when multi-field datasets are accessible, highlighting its superior scalability with richer training information.

```{r}
#| label: fig-improvement
#| fig-cap: "Improvement relative to NO_RF."
#| fig-width: 12
#| fig-height: 7

# compare to which model?
baseline <- "NO_RF"

# average RMSE by Model × field

rmse_df <- rmse_tbl_raw %>%
  dplyr::group_by(Model, `#field`, sim) %>%  
  dplyr::summarise(RMSE = dplyr::first(RMSE), .groups = "drop") %>%
  # Rename models
  dplyr::mutate(
    Model = dplyr::recode(
      Model,
      "Simple_ANN" = "NO_ANN",
      "HalfRANN"   = "SO_ANN",
      "RANN"       = "DO_ANN",
      "HalfRRF"    = "SO_RF",
      "CF"         = "CF",
      "NO_RF"      = "NO_RF",
      .default     = Model
    ),
    field_f = factor(`#field`, levels = c(1,3, 5, 10, 20),
                     labels = c("1 field", "3 fields", "5 fields", "10 fields", "20 fields"))
  )



avg_by <- rmse_df %>%
  dplyr::group_by(Model, field_f) %>%
  dplyr::summarise(mean_rmse = mean(RMSE), .groups = "drop")

# baseline per field 
baseline_by_field <- avg_by %>%
  dplyr::filter(Model == baseline) %>%
  dplyr::select(field_f, baseline_rmse = mean_rmse)

# join and compute % improvement
rel <- avg_by %>%
  dplyr::left_join(baseline_by_field, by = "field_f") %>%
  dplyr::filter(!is.na(baseline_rmse)) %>%
  dplyr::mutate(
    pct_impr = 100 * (baseline_rmse - mean_rmse) / baseline_rmse
    # or: pct_impr = (1 - mean_rmse / baseline_rmse) * 100
  ) %>%
  dplyr::filter(Model != baseline)

# Plot
library(ggplot2)
p_rel <- ggplot(rel, aes(x = field_f, y = pct_impr, fill = Model)) +
  geom_col(position = position_dodge(width = 0.7), width = 0.65) +
  geom_hline(yintercept = 0, linewidth = 0.4) +
  geom_text(aes(label = sprintf("%.1f%%", pct_impr)),
            position = position_dodge(width = 0.7), vjust = -0.4, size = 3, show.legend = FALSE) +
  scale_y_continuous(expand = expansion(mult = c(0.02, 0.10))) +
  labs(x = "# fields", y = sprintf("Improvement vs %s (%%)", baseline), fill = "Model") +
  theme_bw()

p_rel

```
@tbl-best_model_by_field_count highlights the top-performing model for each training size. CF is the dominant method when only a few fields are available, delivering the lowest mean RMSE for 1–5 fields. As field diversity increases, SO_ANN begins to outperform CF, becoming the preferred approach at 10 fields and showing a substantial further accuracy gain at 20 fields.

```{r}

#| label: tbl-best_model_by_field_count
#| tbl-cap: "Best-performing model by field count."
#| tbl-cap-location: bottom
#| echo: false
#| message: false
#| warning: false

library(gt)
library(dplyr)
library(stringr)

best_models <- rmse_summary %>%
  mutate(field_n = as.numeric(str_extract(field_f, "\\d+"))) %>%
  group_by(field_n) %>%
  slice_min(order_by = mean, with_ties = FALSE) %>%
  ungroup() %>%
  arrange(mean, field_n) %>%
  select(Model, field_f, mean)

best_models %>%
  gt() %>%
  tab_header(
    title = "Best models by field count",
    subtitle = "Ordered by lowest mean RMSE"
  ) %>%
  fmt_number(columns = c(mean), decimals = 3) %>%
  cols_label(
    field_f = "# fields",
    mean = "mean RMSE"
  ) %>%
  tab_options(table.font.size = "small")


```



```{r}

#| label: fig-yield-res-1
#| include: false

source(here("codes", "R", "visualize_yield_response_function.R"))
#p1 <- plot_yield_response(model = "CF", n_fields = 1)
p1 <- plot_yield_response("CF", 1, add_title = FALSE)

```

```{r}

#| label: fig-yield-res-2
#| include: false

source(here("codes", "R", "visualize_yield_response_function.R"))
#p2 <- plot_yield_response(model = "CF", n_fields = 3)
p2 <- plot_yield_response("CF", 3, add_title = FALSE)


```

```{r}

#| label: fig-yield-res-3
#| include: false

source(here("codes", "R", "visualize_yield_response_function.R"))
#p3 <- plot_yield_response(model = "CF", n_fields = 5)
p3 <- plot_yield_response("CF", 5, add_title = FALSE)


```



```{r}

#| label: fig-yield-res-4
#| include: false

source(here("codes", "R", "visualize_yield_response_function.R"))
#p4 <- plot_yield_response(model = "SO_ANN", n_fields = 10)
p4 <- plot_yield_response("SO_ANN", 10, add_title = FALSE)


```

```{r}

#| label: fig-yield-res-5
#| include: false

source(here("codes", "R", "visualize_yield_response_function.R"))
#p5 <- plot_yield_response(model = "SO_ANN", n_fields = 20)
p5 <- plot_yield_response("SO_ANN", 20, add_title = FALSE)

```






```{r}
#| include: false

library(patchwork)

```

```{r}
# #| label: fig-yield-res-combined
# #| fig-cap: "Yield response functions for CF and SO_ANN models across different field counts."
# #| echo: false
# #| fig-width: 10
# #| fig-height: 8
# 
# (p1 | p2 | p3) /
# (p4 | p5)
```

```{r}

# #| label: fig-yrf
# #| fig-cap: "Yield response function"
# #| out.width: "100%"
# #| fig-align: "center"
# 
# 
# knitr::include_graphics(here::here("writing/Figures/YRF.pdf"))

```



@fig-yield-res-combined displays predicted and true yield response functions for the best-performing models at each training size. For the CF, shown for 1, 3, and 5 fields, the estimated curves closely track the overall shape of the true response but exhibit a slight upward bias in predicted yield levels. The general curvature, including diminishing returns at higher nitrogen rates, is captured consistently even under data-constrained conditions. As more fields are incorporated, the CF predictions become smoother and more aligned with the true function, reflecting improved generalization.

For the SO_ANN, shown for 10 and 20 fields, the estimated response matches the true curve more closely in both slope and curvature, particularly at higher nitrogen levels where CF tended to flatten. At 20 fields, SO_ANN produces the most accurate approximation overall, with prediction and truth nearly coincident across the nitrogen range. These differences visually reinforce the earlier quantitative results: CF offers strong robustness with sparse multi-field data, while SO_ANN increasingly excels as richer spatial heterogeneity becomes available, enabling superior recovery of the underlying agronomic yield response function.

```{r}
#| label: fig-yield-res-combined
#| fig-cap: "Yield response functions for CF and SO_ANN models across different field counts."
#| echo: false
#| fig-width: 10
#| fig-height: 8

library(patchwork)
library(grid)

# Add small text labels between rows
title_CF <- grid::textGrob(
  "Causal Forest (CF): 1, 3, and 5 fields",
  gp = grid::gpar(fontsize = 13, fontface = "bold")
)
title_SO <- grid::textGrob(
  "Single-Orthogonalized ANN (SO_ANN): 10 and 20 fields",
  gp = grid::gpar(fontsize = 13, fontface = "bold")
)

# Combine with labeled rows
(
  wrap_elements(full = title_CF) /
  (p1 | p2 | p3) /
  wrap_elements(full = title_SO) /
  (p4 | p5)
) +
  plot_annotation(
    #title = "Yield response under different models and field counts",
    theme = theme(plot.title = element_text(size = 15, face = "bold", hjust = 0.5))
  )


```

# Discussion

This study evaluates alternative causal-ML strategies for estimating heterogeneous yield response to a continuous nitrogen treatment in an OFPE setting. CF consistently attains low and tightly concentrated EONR errors with only 1–5 training fields, indicating strong small-sample stability (see @fig-rmse-hist and @fig-avg-performance). SO_ANN improves monotonically with additional fields and ultimately dominates once ≥10 fields are available, achieving the lowest mean RMSE overall (e.g., 11.2 at 10 fields and 8.5 at 20 fields). These complementary strengths that CF is preferred under data-constrained conditions, whereas SO_ANN becomes the method of choice as multi-field datasets accumulate (see @tbl-best_model_by_field_count).

Methodologically, the contrast between single-orthogonalization and double-orthogonalization is central. For continuous treatments, the double-orthogonal residualization of the treatment does not generally preserve the relevant heterogeneous response structure. Our simulations reflect this: DO_ANN performs poorly and does not benefit from larger samples. In contrast, single-orthogonalization—residualizing only the outcome to remove baseline variation from covariates $W$—retains the full interaction between $X$ and $N$, allowing the final learner to recover the nonlinearity and heterogeneity of the response function. The extended CF strikes a different balance: by expressing $N$ through a spline basis and orthogonalizing the basis components, it restores a linear-in-parameters score that CF can learn reliably. 


# Conclusion

By combining orthogonalization with flexible final-stage learners, we demonstrate how to preserve the strengths of causal inference while accommodating continuous treatments and agronomic heterogeneity.

Across 500 simulated test fields, two approaches consistently dominate: the extended causal forest and the single-orthogonalized neural network. CF is highly robust in small-sample environments, achieving strong accuracy even with training data from just one to five fields. As the number of training fields increases, SO_ANN increasingly improves as a result of spatial and agronomic diversity, ultimately delivering the best predictive accuracy of all methods once ten or more fields are available. In contrast, non-orthogonalized baselines require larger samples to become competitive and the doubly-orthogonalized ANN performs poorly in all conditions due to the incompatibility between double-orthogonalization and continuous treatments. Methodologically, our results highlight the importance of aligning the orthogonalization strategy with the structure of the treatment variable in causal ML applications.




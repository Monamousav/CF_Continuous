---
title: "Estimation of the effect of a continuous treatment variable"
format:
  html: default
execute:
  echo: false
bibliography: references.bib
---

```{r}
#| include: false
library(data.table)
library(dplyr)
library(flextable)
library(officer)
```

# Introduction

+ Background
+ Why we need the methods we are proposing? Motivation.
+ Summary of findings 

# Method

We run Monte Carlo simulation to test the performance of ...

## Data generation


## Models and estimation procedures

<span style = "color: blue;"> Use math here to explain orthogonalization. This is where you explain what SO and DO in general. The sections below add specifics to the individual approaches we take. You also need to provide contexts (past studies). </span>


The standard approach in the precision-agriculture literature for estimating site-specific input response is to model the outcome level directly as a function of the treatment/input and covariates—e.g., estimate $Y=f(I,C)$ where $I$ is the input (like nitrogen rate) and $C$ are field characteristics—and then derive optimal input decisions from that estimated function. This is what is typically done in the recent on-farm precision experimentation literature (e.g., @barbosa2020risk; @barbosa2020modeling; @krause2020random; @gardner2021economic).
The problem is that these methods optimize prediction of yield levels—not the causal effect of changing the input. Good level prediction at the observed input does not guarantee accurate recovery of the response function needed for optimal input choice, because the fitted $f(I,C)$ can conflate input variation with confounding and misattribute the drivers of yield variation.

To identify causal effects rather than just predict levels in the single binary treatment case, researchers use double/debiased machine learning (DML), which introduces orthogonalization so the treatment-effect estimator is insensitive to small errors in the auxiliary fits (I need citation here). 
Consider the structural model (Model 1):

```{=tex}
\begin{align*}
Y=\theta(X) T+g(X, W)+v
\end{align*}
```

where $Y$ is the outcome, $T \in\{0,1\}$  is the treatment, $X$ covariates that also modify the treatment effect $\theta(X)$, $W$ additional predictors of $Y$, and $v$ is noise. 
Orthogonalization estimates $f(X, W)=\mathbb{E}[Y \mid X, W]$ and $h(X, W)=\mathbb{E}[T \mid X, W]$, forms residuals $Y_{\mathrm{res}}=Y-f(X,W)$ and $T_{\mathrm{res}}=T-h(X, W)$, and then relates $Y_{\mathrm{res}}$ to $T_{\mathrm{res}}$. Because this score is Neyman-orthogonal and is typically implemented with cross-fitting, small errors in $f$ and $h$ do not induce first-order bias in $\theta(X)$.

The identifying algebra in Model 1 follows from linearity in $T$:

```{=tex}
\begin{align*}
\mathbb{E}[Y \mid X, W]=\theta(X) \mathbb{E}[T \mid X, W]+g(X, W)=\theta(X) h(X, W)+g(X, W)
\end{align*}
```

so

```{=tex}
\begin{align*}
Y_{\mathrm{res}}=Y-\mathbb{E}[Y \mid X, W]=\theta(X)(T-h(X, W))+v=\theta(X) T_{\mathrm{res}}+v
\end{align*}
```

Thus regressing $Y_{\mathrm{res}}$ on $T_{\mathrm{res}}$ recovers $\theta(X)$ up to noise.

In the on farm precision experiment (OFPE) setting, the ideas behind DML have shown promise for causal effect estimation. @kakimoto2022causal applied this framework to estimate heterogeneous treatment effects of nitrogen on yield, but their implementation was limited to a single-year experiment from one field and treated heterogeneity in a discrete way. They defined multiple “treatments” as increases from the lowest nitrogen rate to each higher rate, which is natural when there are only five or six fixed target rates per field. When data are pooled across fields with differing targets, however, applied nitrogen rates become effectively continuous, and the standard residualization of the treatment in DML does not extend cleanly to that setting.

Consider the continuous-treatment setting, where the structural model is

```{=tex}
\begin{align*}
Y=\theta(X, T)+g(X, W)+v
\end{align*}
```

with $T$ (e.g., nitrogen rate) continuous, and $X$ heterogeneity-inducing covariates that interact with $T$ through the flexible response function $\theta(X, T)$. In this case double-orthogonalization fails because $\theta$ is a general (nonlinear and interaction-rich) function of $T$. Residualizing the treatment by replacing $T$ with $T-\mathbb{E}[T \mid X, W]$ does not produce a valid adjustment for the heterogeneous effect. Formally,

```{=tex}
\begin{align*}
\theta(X, T)-\mathbb{E}[\theta(X, T) \mid X, W] \neq \theta(X, T-\mathbb{E}[T \mid X, W])
\end{align*}
```

and the clean decomposition that underlies double-orthogonalization in the linear-in-$T$ case breaks down. 
Instead, we apply single-orthogonalization (SO). We first estimate $f(X, W)=\mathbb{E}[Y \mid X, W]$ and form the residual $Y_{\mathrm{res}}=Y-f(X, W)$, and then recover $\theta(X, T)$ by flexibly modeling  $Y_{\mathrm{res}}$ as a function of $(X,T)$. This approach removes baseline variation due to $g(X,W)$ while preserving the full interaction structure between $X$ and the continuous treatment.

To study and compare these ideas systematically, we adopt a single underlying structural model and then implement alternative orthogonalization schemes.

The model of interest throughout is (Model 3)

```{=tex}
\begin{align*}
y = f(X, N) + g(W) + v
\end{align*}
```

where $y$ is crop yield, $N$ is the continuous nitrogen rate, $X$ is a set of covariates that induce heterogeneity in the impact of $N$ on $y$, and $W$ is a set of covariates that affect $y$ but do not interact with $N$. 
We implement three estimation strategies—double-orthogonalized, single-orthogonalized, and non-orthogonalized—that differ only in how (and whether) the outcome and treatment are residualized before the final fit. Orthogonalization means fitting a variable on its conditioning covariates and using the residual (observed minus predicted) so that the subsequent learner focuses on variation not explained by those covariates; all residualizations are performed with random forests. In the double-orthogonalized approach, both $y$ and $N$ are residualized with respect to $(X,W)$ and the final learner is trained on those residuals. In the single-orthogonalized approach, only $y$ is residualized and the raw $N$ is retained. The non-orthogonalized specification skips residualization entirely and fits the learner directly to $(y,N,X,W)$. For each strategy we consider three final-stage learners—neural networks (NN), random forests (RF), and causal forests (CF)—to evaluate how the orthogonalization scheme and learning algorithm interact in recovering heterogeneous nitrogen response.


The table below shows all the cases we consider.


```{r}
#| include: false
f_est <-
  data.table(
    "Approach Label" = c("DO-NN", "SO-RFF", "SO-NN", "DO-CF","NO-NN", "NO-RF"),
    "Approach Type" = c("Double-orthogonalized", "Single-orthogonalized", "Single-orthogonalized", "Double-orthogonalized", "Non-orthogonalized", "Non-orthogonalized"),
    "Y-orthogonalization" = c("Randome Forest", "Randome Forest", "Randome Forest", "Randome Forest", "NA", "NA"),
    "T-orthogonalization" = c("Randome Forest", "NA", "NA", "Randome Forest", "NA", "NA"),
    "Final stage" = c("Neural Network", "Random Forest", "Neural Network", "Causal Forest", "Neural Network", "Random Forest")
  ) %>%
  flextable() %>%
  fontsize(size = 16, part = "all") %>%
  autofit()

flextable::save_as_image(f_est, here::here("writing/f_est.png"), res = 300)
```

```{r}
#| echo: false
#| fig-width: 4
#| out-width: 4in
knitr::include_graphics(here::here("writing/f_est.png"))
```

### Single-orthogonalized NN (SO-NN) and (SO-RF)

In the single-orthogonalized framework only the baseline component $g(W)$ is partialed out before the final-stage estimation; the continuous treatment $N$ and heterogeneity-inducing covariates $X$ remain in their original form. Formally, starting from Model 3, we first estimate $g(W)=\mathbb{E}[y \mid W]$ using a cross-fitted random forest and form the residualized outcome

```{=tex}
\begin{align*}
\tilde{y}=y-\widehat{g}(W)
\end{align*}
```

The second stage fits a flexible model of $\tilde{y}$ on $(X,N)$ to recover the heterogeneous response function $f(X,N)$. The distinction between SO-NN and SO-RF is only in the choice of the final learner: SO-NN uses a neural network, while SO-RF uses a random forest. In both cases the learner estimates $\hat{f}(X, N)$, which is then used to predict yield under candidate nitrogen rates. Marginal profit is computed over a grid of $N$ values and the economically optimal nitrogen rate is selected as the one that maximizes 

```{=tex}
\begin{align*}
\widehat{\mathrm{EONR}}=\arg \max _N\left[P_{\text {Corn }} \times \hat{f}(X, N)-P_N \times N\right]
\end{align*}
```
where $P_{\text {Corn }}$ is the corn price, $P_{\text {N }}$ is the nitrogen price, and $\hat{f}$ is the estimated yield response.

### Double-orthogonalized CF (DO-CF) and ANN (DO-ANN) 

#### DO‑CF (continuous)

To adapt double-orthogonalization to the model of interest (Model 3), we estimate the heterogeneous response $f(X,N)$ by expanding the continuous treatment $N$ in a set of spline basis functions and allowing their coefficients to vary with the heterogeneity-inducing covariates $X$:

```{=tex}
\begin{align*}
y=\sum_{k=1}^K \theta_k(X) S_k(N)+g(X, W)+v
\end{align*}
```

Although double-orthogonalization generally fails when $f(X,N)$ is an arbitrary nonlinear function of $N$, this spline-based formulation restores the key linear-in-parameter structure that makes DO valid, because the model is linear in the transformed treatment basis $\left\{S_k(N)\right\}$. Residualizing each basis component and the outcome with respect to $(X,W)$ then yields a clean decomposition analogous to the linear-in-$T$ case. Let $\widetilde{S}_k(N)=S_k(N)-\mathbb{E}\left[S_k(N) \mid X, W\right]$. Then

```{=tex}
\begin{align*}
\tilde{y}=y-\mathbb{E}[y \mid X, W]=\sum_{k=1}^K \theta_k(X) \widetilde{S}_k(N)+v,
\end{align*}
```

so causal forest can nonparametrically estimate each heterogeneous weight $\theta_k(X)$  by relating the residualized outcome to the residualized spline bases. This Neyman-orthogonal (R-learner) style decomposition preserves the ability to capture complex, heterogeneous, and nonlinear treatment effects while inheriting the robustness of double-orthogonalization: small errors in the nuisance fits for $g(X,W)$ or the conditional means of the bases do not induce first-order bias in $\theta_k(X)$. The estimated $\theta_k(X)$ then combine to recover the full heterogeneous response function and, by differencing across candidate $N$ values, site-specific treatment effects and economically optimal nitrogen rates.

#### DO‑ANN

In the DO-ANN approach, we apply double-orthogonalization to Model 3 by residualizing both the outcome $y$ and the continuous treatment $N$ with respect to $(X, W)$ using cross-fitted random forests:

```{=tex}
\tilde{y}=y-\widehat{\mathbb{E}}[y \mid X, W], \quad \tilde{N}=N-\widehat{\mathbb{E}}[N \mid X, W]
\end{align*}
```

The final-stage learner is a neural network that models $\tilde{y}$ as a linear function of $\tilde{N}$, with heterogeneity in the slope captured by covariates $X$:

```{=tex}
\hat{\tilde{y}}=\hat{\theta}(X) \cdot \tilde{N}
\end{align*}
```

This structure preserves the linear-in-treatment-residual form required for valid double-orthogonalization while allowing $\hat{\theta}(X)$ to vary flexibly across sites. The neural network receives $X$ as input and outputs the site-specific treatment effect.

After training, we reconstruct $\hat{f}(X, N)$ over a grid of candidate nitrogen rates by evaluating the model across values of $N$, and select the economically optimal nitrogen rate.

### Non-orthogonalized ANN (NO-NN) and RF (NO-RF)

In the non‑orthogonalized setting, neither the outcome variable nor the treatment is residualized prior to estimation. Instead, the model directly fits the observed yield on both the nitrogen rate and covariates. We estimate the response function $\hat{Y}=\hat{f}(X, N)$ using the original values of $Y$, $N$ and $X$, without any orthogonalization. As in the single‑orthogonalized approaches, the function $\hat{f}(X, N)$ is estimated non‑parametrically. The difference between NO‑NN and NO‑RF lies in the choice of final-stage learner: NO‑NN uses a neural network, while NO‑RF relies on a random forest. Once the yield response function is estimated, the EONR is determined using the same profit-maximization procedure described in earlier models.


## Performance Measurement

For each simulation round, we evaluate model performance by calculating the root mean squared error (RMSE) of EONR predictions and the associated profit loss for each method. The RMSE of EONR predictions in a given simulation round is computed as follows:


```{=tex}
\begin{align*}
RMSE_{EONR}=\sqrt{\frac{1}{n} \sum_{i=1}^n\left(\widehat{EONR}_i-EONR_i\right)^2}
\end{align*}
```

To assess the predictive accuracy and economic relevance of these modeling approaches, we design a set of simulation experiments that reflect realistic agronomic conditions. These simulations serve as a controlled environment for evaluating model performance in estimating yield response functions and recommending optimal nitrogen rates.

## Simulation Scenarios

To evaluate the performance of the proposed models we design Monte Carlo simulations that mimic a typical on-farm precision experimentation (OFPE) setting. These simulations examine the ability of each method to accurately estimate the site-specific yield response to nitrogen application and the EONR under varying data availability conditions.

Our simulation consists of 500 independently generated fields, each representing a distinct agronomic environment with field-specific parameters that shape the crop’s response to nitrogen application. These parameters vary systematically across fields to introduce heterogeneity in yield responses. The true yield response in each field follows a quadratic-plateau functional form, a widely used structure for modeling nitrogen response curves in agronomic studies.

For each simulation round, one field is selected as the test field. The remaining fields are used to construct the training set in three scenarios: (1) one-field training, (2) five-field training, and (3) ten-field training. These scenarios allow us to assess how model performance changes with the size and diversity of training data. Within each round, the model is trained on the designated number of training fields and then used to estimate the yield response function and the site-specific EONR for the test field.

The experimental design within each field consists of randomized nitrogen treatment rates applied across plots. The nitrogen rates vary between fields and are drawn to reflect realistic trial setups that span a range of agronomic optima. For evaluation purposes, we use a grid of candidate nitrogen rates to compute predicted yield and associated marginal profit for each observation. The predicted EONR is the nitrogen rate that maximizes predicted profit.

# Simulation Results 

We compare six estimators—CF, SO_ANN, DO_ANN, NO_ANN, SO_RF, and NO_RF—over 500 Monte-Carlo replications and three training sizes (1, 5, and 10 fields). Each replication contains 1,440 evaluation units. Accuracy is assessed by the RMSE of the estimated economically optimal nitrogen rate (EONR) with respect to the true (simulated) EONR. Figures 1–4 summarize errors; Figure 5 panels illustrate representative yield–N response functions that help interpret the error patterns.


```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)


suppressPackageStartupMessages(library(here))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(stringr))

# load data
rmse_tbl_raw <- readRDS(here::here("results", "rmse_tbl_raw.rds"))
```

@fig-rmse-hist shows a clear ordering across methods. CF concentrates tightly at low RMSE for all training sizes, indicating both accuracy and stability. SO_ANN shifts steadily left as the number of fields increases, with visibly narrower spreads at 5 and 10 fields. In contrast, NO_ANN exhibits a heavy right tail with a single field but collapses toward the leading methods at 5–10 fields, showing its sensitivity to sample size. SO_RF improves with more fields yet remains less accurate than SO_ANN or CF. NO_RF has broad, right-shifted distributions and is a weak baseline. DO_ANN performs poorly across all sizes, with distributions centered at very high RMSE and little improvement from additional fields.

```{r}
#| label: fig-rmse-hist
#| fig-cap: "RMSE histograms by model and number of fields."
#| fig-width: 12
#| fig-height: 7

# model names: row labels in the graph
model_to_row <- c(
  "Simple_ANN" = "No_ANN",
  "RANN"       = "DO_ANN",
  "NO_RF"      = "NO_RF",
  "HalfRANN"   = "SO_ANN",
  "HalfRRF"    = "SO_RF",
  "CF"         = "CF"
)

rmse_plot_df <- rmse_tbl_raw %>%
  dplyr::filter(ok_rows, Model %in% names(model_to_row)) %>%
  dplyr::mutate(
    row_lab = dplyr::recode(Model, !!!model_to_row),
    col_lab = factor(`#field`,
                     levels = c(1, 5, 10),
                     labels = c("Number of fields: 1",
                                "Number of fields: 5",
                                "Number of fields: 10"))
  )

# consistent axes
xmin <- 0
nice_ceiling <- function(x, base = 5) base * ceiling(x / base)
xmax <- nice_ceiling(max(rmse_plot_df$RMSE, na.rm = TRUE), base = 5)
bins <- 25
binwidth <- (xmax - xmin) / bins

p <- ggplot(rmse_plot_df, aes(x = RMSE)) +
  geom_histogram(binwidth = binwidth, boundary = 0, closed = "left") +
  facet_grid(row_lab ~ col_lab, scales = "fixed") +
  scale_x_continuous(limits = c(xmin, xmax),
                     breaks = seq(xmin, xmax, length.out = 6),
                     expand = c(0, 0)) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) +
  labs(x = "RMSE by iteration", y = "Count") +
  theme_bw() +
  theme(
    strip.text = element_text(face = "bold", size = 16),
    axis.title = element_text(size = 14)
  )

p
```

Mean RMSE by model and training size (@fig-avg-performance and @tbl-mean-rmse-heatmap) confirms these patterns. CF is consistently low (≈15 across sizes). SO_ANN improves monotonically as data increase and attains the best mean at 10 fields (≈11). NO_ANN is unusable with a single field (mean ≈82) but becomes competitive by 5–10 fields (≈14 and ≈13). SO_RF yields moderate means (≈22 at 10 fields), while NO_RF remains highest among the non-ANN baselines (≈36–40). DO_ANN is uniformly poor (≈90–93). The heatmap (Fig. 3) prints the means in each cell and emphasizes the strong gains from orthogonalization (SO_ANN and CF) relative to their non-orthogonal counterparts.


```{r}
#| label: fig-avg-performance
#| fig-cap: "Average performance by model and number of fields."
#| fig-width: 12
#| fig-height: 7

rmse_df <- rmse_tbl_raw %>%
  dplyr::group_by(Model, `#field`, sim) %>%  
  dplyr::summarise(RMSE = dplyr::first(RMSE), .groups = "drop") %>%
  # Rename models
  dplyr::mutate(
    Model = dplyr::recode(
      Model,
      "Simple_ANN" = "NO_ANN",
      "HalfRANN"   = "SO_ANN",
      "RANN"       = "DO_ANN",
      "HalfRRF"    = "SO_RF",
      "CF"         = "CF",
      "NO_RF"      = "NO_RF",
      .default     = Model
    ),
    field_f = factor(`#field`, levels = c(1, 5, 10),
                     labels = c("1 field", "5 fields", "10 fields"))
  )

# 3) summary stats 
rmse_summary <- rmse_df %>%
  dplyr::group_by(Model, field_f) %>%
  dplyr::summarise(
    n      = dplyr::n(),
    mean   = mean(RMSE, na.rm = TRUE),
    median = median(RMSE, na.rm = TRUE),
    sd     = sd(RMSE, na.rm = TRUE),
    se     = sd / sqrt(n),
    lower  = mean + qt(0.025, df = n - 1) * se,
    upper  = mean + qt(0.975, df = n - 1) * se,
    .groups = "drop"
  )

# 
p_mean <- ggplot(rmse_summary, aes(x = field_f, y = mean, group = Model, color = Model)) +
  geom_line(linewidth = 0.7, alpha = 0.8) +
  geom_point(size = 3) +
  labs(x = "# fields", y = "Mean RMSE", color = "Model") +
  theme_bw() +
  theme(
    axis.title = element_text(size = 13),
    strip.text = element_text(face = "bold")
  )

p_mean


# ggsave(here::here("results","rmse_mean_by_model_fields.png"), p_mean, width = 9, height = 5, dpi = 300)
```

```{r}
#| label: tbl-mean-rmse-heatmap
#| tbl-cap: "Mean RMSE by model and number of fields."
#| tbl-cap-location: bottom
#| echo: false
#| message: false
#| warning: false

library(dplyr)
library(tidyr)
library(gt)
library(scales)

# Range for coloring
fill_min <- floor(min(rmse_summary$mean, na.rm = TRUE))
fill_max <- ceiling(max(rmse_summary$mean, na.rm = TRUE))

# Wide table of means
rmse_table <- rmse_summary %>%
  select(Model, field_f, mean) %>%
  mutate(mean = round(mean, 1)) %>%
  pivot_wider(names_from = field_f, values_from = mean) %>%
  arrange(Model)

# Light → mid-blue palette
pal <- col_numeric(
  palette = c("#f7fbff", "#6baed6"),
  domain  = c(fill_min, fill_max)
)

# HTML colorbar legend to place under the table
legend_html <- sprintf(
  '<div style="display:flex;align-items:center;gap:8px;margin-top:6px;">
     <span style="font-size:12px;color:#444;">Low (%.1f)</span>
     <div style="flex:1;height:10px;background:linear-gradient(to right,#f7fbff,#6baed6);
                 border:1px solid #ddd;border-radius:3px;"></div>
     <span style="font-size:12px;color:#444;">High (%.1f)</span>
   </div>', fill_min, fill_max)

rmse_table %>%
  gt(rowname_col = "Model") %>%
  data_color(
    columns = c(`1 field`, `5 fields`, `10 fields`),
    colors  = pal
  ) %>%
  fmt_number(columns = c(`1 field`, `5 fields`, `10 fields`), decimals = 1) %>%
  cols_label(
    `1 field`  = "1 field",
    `5 fields` = "5 fields",
    `10 fields`= "10 fields"
  ) %>%
  tab_options(
    table.font.size = px(13),
    data_row.padding = px(6),
    column_labels.font.weight = "bold"
  ) %>%
  tab_source_note(source_note = gt::html(legend_html))
```

Using NO_RF as a baseline, CF reduces mean RMSE by roughly 58–61 % across training sizes (@fig-improvement). SO_ANN delivers comparable or larger gains (≈54–69 %), with the largest improvement at 10 fields. SO_RF provides modest gains (≈38–42 %). NO_ANN is worse than the baseline with 1 field but improves substantially at 5–10 fields (≈63–65 % reduction). DO_ANN is consistently inferior to the baseline at all sizes.

```{r}
#| label: fig-improvement
#| fig-cap: "Improvement relative to NO_RF."
#| fig-width: 12
#| fig-height: 7

# compare to which model?
baseline <- "NO_RF"

# average RMSE by Model × field

rmse_df <- rmse_tbl_raw %>%
  dplyr::group_by(Model, `#field`, sim) %>%  
  dplyr::summarise(RMSE = dplyr::first(RMSE), .groups = "drop") %>%
  # Rename models
  dplyr::mutate(
    Model = dplyr::recode(
      Model,
      "Simple_ANN" = "NO_ANN",
      "HalfRANN"   = "SO_ANN",
      "RANN"       = "DO_ANN",
      "HalfRRF"    = "SO_RF",
      "CF"         = "CF",
      "NO_RF"      = "NO_RF",
      .default     = Model
    ),
    field_f = factor(`#field`, levels = c(1, 5, 10),
                     labels = c("1 field", "5 fields", "10 fields"))
  )



avg_by <- rmse_df %>%
  dplyr::group_by(Model, field_f) %>%
  dplyr::summarise(mean_rmse = mean(RMSE), .groups = "drop")

# baseline per field 
baseline_by_field <- avg_by %>%
  dplyr::filter(Model == baseline) %>%
  dplyr::select(field_f, baseline_rmse = mean_rmse)

# join and compute % improvement
rel <- avg_by %>%
  dplyr::left_join(baseline_by_field, by = "field_f") %>%
  dplyr::filter(!is.na(baseline_rmse)) %>%
  dplyr::mutate(
    pct_impr = 100 * (baseline_rmse - mean_rmse) / baseline_rmse
    # or: pct_impr = (1 - mean_rmse / baseline_rmse) * 100
  ) %>%
  dplyr::filter(Model != baseline)

# Plot
library(ggplot2)
p_rel <- ggplot(rel, aes(x = field_f, y = pct_impr, fill = Model)) +
  geom_col(position = position_dodge(width = 0.7), width = 0.65) +
  geom_hline(yintercept = 0, linewidth = 0.4) +
  geom_text(aes(label = sprintf("%.1f%%", pct_impr)),
            position = position_dodge(width = 0.7), vjust = -0.4, size = 3, show.legend = FALSE) +
  scale_y_continuous(expand = expansion(mult = c(0.02, 0.10))) +
  labs(x = "# fields", y = sprintf("Improvement vs %s (%%)", baseline), fill = "Model") +
  theme_bw()

p_rel

```

Predicted yield–N curves (@fig-yield-res-1 through @fig-yield-res-5) for five randomly selected analysis units clarify why methods differ. SO_ANN produces agronomically plausible, monotone-concave responses with clear plateaus (ten-field panel), yielding interior optima and low EONR error. NO_ANN resembles SO_ANN once trained on 5–10 fields, but with a single field it is noticeably noisier, aligning with its heavy-tailed errors. Tree-based models (NO_RF and SO_RF) generate step-like, piecewise-constant responses; the implied EONR can only move at jump points, which coarsens the optimum and limits accuracy. DO_ANN often yields nearly linear increasing curves across the tested N range, pushing the optimum to the upper bound and explaining its uniformly high error. Although CF is not naturally expressed as per-unit yield curves, its error distribution indicates high stability across all sizes.

```{r}
#| label: fig-yield-res-1
#| fig-cap: "Yield response function (NO_ANN; 5 fields)."
#| fig-cap-location: bottom
#| fig-width: 12
#| fig-height: 7
#| message: false
#| warning: false

source(here("codes", "R", "visualize_yield_response_function.R"))


plot_yield_response(model = "NO_ANN", n_fields = 5)     # uses N
```
```{r}
#| label: fig-yield-res-2
#| fig-cap: "Yield response function (DO_ANN; 5 fields)."
#| fig-width: 12
#| fig-height: 7

source(here("codes", "R", "visualize_yield_response_function.R"))
plot_yield_response(model = "DO_ANN", n_fields = 5)     # uses N_tilde 

```

```{r}
#| label: fig-yield-res-3
#| fig-cap: "Yield response function SO_ANN (SO_ANN; 10 fields)."
#| fig-width: 12
#| fig-height: 7
#| 
source(here("codes", "R", "visualize_yield_response_function.R"))
plot_yield_response(model = "SO_ANN", n_fields = 10) 

```

```{r}
#| label: fig-yield-res-4
#| fig-cap: "Yield response function NO_RF (NO_RF; 5 fields)."
#| fig-width: 12
#| fig-height: 7
#| 
source(here("codes", "R", "visualize_yield_response_function.R"))
plot_yield_response(model = "NO_RF", n_fields = 5) 

```

```{r}
#| label: fig-yield-res-5
#| fig-cap: "Yield response function SO_RF (SO_RF; 5 fields)."
#| fig-width: 12
#| fig-height: 7
#| 
source(here("codes", "R", "visualize_yield_response_function.R"))
plot_yield_response(model = "SO_RF", n_fields = 5) 
```


# Conclusion

Across 500 replications, SO_ANN (with ≥10 fields) and CF deliver the best overall accuracy. Orthogonalization is critical: SO_ANN and CF are consistently superior to NO_ANN and NO_RF, especially in small-to-moderate samples. NO_ANN becomes competitive only once training data reach at least five fields. Tree-based baselines remain substantially less accurate, and the doubly-orthogonal ANN (DO_ANN) is inappropriate for this setting because it fails to recover the saturating response shape. These findings are robust across training sizes and are consistent.



